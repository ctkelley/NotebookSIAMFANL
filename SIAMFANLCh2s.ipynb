{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.8 Solvers for Chapter 2\n",
    "\n",
    "Contents for Section 2.8\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "- [nsol.jl](#Section-2.8.1:-nsol.jl)\n",
    "\n",
    "- [H-equation Revisited](#Section-2.8.2:-H-Equation-revisited)\n",
    "\n",
    "- [More on the Two-Point BVP](#Section-2.8.3:-More-on-the-Two-Point-BVP)\n",
    "\n",
    "- [Shamanskii for the Convection-Diffusion Problem](#Section-2.8.4:-Shamanskii-for-the-Convection-Diffusion-Problem)\n",
    "\n",
    "- [ptcsol.jl](#Section-2.8.5:-ptcsol.jl)\n",
    "\n",
    "- [More on the Buckling Beam](#Benchmarking-the-Buckling-Beam)\n",
    "\n",
    "- [Section 2.9: Projects](#Section-2.9-Projects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the pattern of Chapter 1 and present two solvers, a Newton code and a $\\ptc$ code. Both codes are for systems of equations and use direct methods to compute the step. We returned the solution history for the simple two dimensional example in Section 2.6, but will not do that again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8.1: nsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsol.jl__ solves systems of nonlinear equations and computes the Newton step with direct linear solvers. Let's look at the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mi \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mheq \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mPDE \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22miPDE i\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrted tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mc\u001b[0m\u001b[1mo\u001b[22mde tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = klfact,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "\\end{verbatim}\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2022\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is a vector of size N\n",
       "\n",
       "You should store it as (N) and design F! to use vectors of size (N).  If you use (N,1) consistently instead, the solvers may work, but I make no guarantees.\n",
       "\n",
       "\n",
       "\\item FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "\n",
       "(FP,FS, x) must be the argument list, even if FP does not need FS.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare   FPS as Float64, Float32, or Float16 and nsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "solver: default = \"newton\"\n",
       "\n",
       "Your choices are \"newton\" or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham: default = 5 (ie Newton)\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "I made sham=1 the default for scalar equations. For systems I'm more aggressive and want to invest as little energy in linear algebra as possible. So the default is sham=5.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "resdec: default = .1\n",
       "\n",
       "This is the target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in either of F! or J!, you must use in in the  calling sequence of both.\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  nsol will use backslash to compute the Newton step. I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "If you want to manage your own factorization within your Jacobian  evaluation function, then set\n",
       "\n",
       "jfact = nofact\n",
       "\n",
       "and nsol will not attempt to factor your Jacobian. That is also what happens when klfact does not know what to do. Your Jacobian is sent directly to Julia's {\\textbackslash}  operation\n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "\\begin{verbatim}\n",
       "    FPF = PrepareJac!(FPS, FS, x, ItRules)\n",
       "\\end{verbatim}\n",
       "FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "\\end{itemize}\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\\begin{verbatim}\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "\\end{verbatim}\n",
       "– solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iterations + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\subsubsection{Examples for nsol}\n",
       "\\paragraph{World's easiest problem example. Test 64 and 32 bit Jacobians. No meaningful difference in the residual histories or the converged solutions.}\n",
       "\\begin{verbatim}\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       return fv\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2); fv=zeros(2); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "julia> nout=nsol(f!,x,fv,jv; sham=1);\n",
       "julia> nout32=nsol(f!,x,fv,jv32; sham=1);\n",
       "julia> [nout.history nout32.history]\n",
       "5×2 Matrix{Float64}:\n",
       " 1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01\n",
       " 1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03265e-05\n",
       " 1.46388e-11  1.45995e-11\n",
       "\n",
       "julia> [nout.solution nout.solution-nout32.solution]\n",
       "2×2 Array{Float64,2}:\n",
       " -7.39085e-01  -5.48450e-14\n",
       "  2.30988e+00  -2.26485e-14\n",
       "\\end{verbatim}\n",
       "\\paragraph{H-equation example. I'm taking the sham=5 default here, so the convergence is not quadratic. The good news is that we evaluate the Jacobian only once.}\n",
       "\\begin{verbatim}\n",
       "julia> n=16; x0=ones(n); FV=ones(n); JV=ones(n,n);\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "4-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 2.75227e-05\n",
       " 2.35817e-07\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = klfact,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "```\n",
       "\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2022\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is a vector of size N\n",
       "\n",
       "    You should store it as (N) and design F! to use vectors of size (N).  If you use (N,1) consistently instead, the solvers may work, but I make no guarantees.\n",
       "  * FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "    So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "\n",
       "    (FP,FS, x) must be the argument list, even if FP does not need FS.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare   FPS as Float64, Float32, or Float16 and nsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "    BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "solver: default = \"newton\"\n",
       "\n",
       "Your choices are \"newton\" or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham: default = 5 (ie Newton)\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "I made sham=1 the default for scalar equations. For systems I'm more aggressive and want to invest as little energy in linear algebra as possible. So the default is sham=5.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "resdec: default = .1\n",
       "\n",
       "This is the target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in either of F! or J!, you must use in in the  calling sequence of both.\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  nsol will use backslash to compute the Newton step. I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "If you want to manage your own factorization within your Jacobian  evaluation function, then set\n",
       "\n",
       "jfact = nofact\n",
       "\n",
       "and nsol will not attempt to factor your Jacobian. That is also what happens when klfact does not know what to do. Your Jacobian is sent directly to Julia's \\  operation\n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "```\n",
       "    FPF = PrepareJac!(FPS, FS, x, ItRules)\n",
       "```\n",
       "\n",
       "FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "  * A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "```\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "```\n",
       "\n",
       "– solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iterations + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "---\n",
       "\n",
       "### Examples for nsol\n",
       "\n",
       "#### World's easiest problem example. Test 64 and 32 bit Jacobians. No meaningful difference in the residual histories or the converged solutions.\n",
       "\n",
       "```jldoctest\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       return fv\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2); fv=zeros(2); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "julia> nout=nsol(f!,x,fv,jv; sham=1);\n",
       "julia> nout32=nsol(f!,x,fv,jv32; sham=1);\n",
       "julia> [nout.history nout32.history]\n",
       "5×2 Matrix{Float64}:\n",
       " 1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01\n",
       " 1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03265e-05\n",
       " 1.46388e-11  1.45995e-11\n",
       "\n",
       "julia> [nout.solution nout.solution-nout32.solution]\n",
       "2×2 Array{Float64,2}:\n",
       " -7.39085e-01  -5.48450e-14\n",
       "  2.30988e+00  -2.26485e-14\n",
       "```\n",
       "\n",
       "#### H-equation example. I'm taking the sham=5 default here, so the convergence is not quadratic. The good news is that we evaluate the Jacobian only once.\n",
       "\n",
       "```jldoctest\n",
       "julia> n=16; x0=ones(n); FV=ones(n); JV=ones(n,n);\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "4-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 2.75227e-05\n",
       " 2.35817e-07\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\u001b[39m\n",
       "\u001b[36m             maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\u001b[39m\n",
       "\u001b[36m             dx = 1.e-7, armfix=false, \u001b[39m\n",
       "\u001b[36m             pdata = nothing, jfact = klfact,\u001b[39m\n",
       "\u001b[36m             printerr = true, keepsolhist = false, stagnationok=false)\u001b[39m\n",
       "\n",
       "  )\n",
       "\n",
       "  C. T. Kelley, 2022\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: nsol\n",
       "\n",
       "  You must allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •  F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "       your preallocated storage for the function.\n",
       "       So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "    •  x0: initial iterate\n",
       "\n",
       "    •  FS: Preallocated storage for function. It is a vector of size N\n",
       "       You should store it as (N) and design F! to use vectors of size\n",
       "       (N). If you use (N,1) consistently instead, the solvers may work,\n",
       "       but I make no guarantees.\n",
       "\n",
       "    •  FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    •  J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "       your preallocated storage for the Jacobian. If you leave this out\n",
       "       the default is a finite difference Jacobian.\n",
       "       So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x).\n",
       "       (FP,FS, x) must be the argument list, even if FP does not need FS.\n",
       "       One reason for this is that the finite-difference Jacobian does\n",
       "       and that is the default in the solver.\n",
       "\n",
       "    •  Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "       full precision functions and linear algebra in any precision you\n",
       "       want. You can declare FPS as Float64, Float32, or Float16 and nsol\n",
       "       will do the right thing if YOU do not destroy the declaration in\n",
       "       your J! function. I'm amazed that this works so easily. If the\n",
       "       Jacobian is reasonably well conditioned, you can cut the cost of\n",
       "       Jacobian factorization and storage in half with no loss. For large\n",
       "       dense Jacobians and inexpensive functions, this is a good deal.\n",
       "       BUT ... There is very limited support for direct sparse solvers in\n",
       "       anything other than Float64. I recommend that you only use Float64\n",
       "       with direct sparse solvers unless you really know what you're\n",
       "       doing. I have a couple examples in the notebook, but watch out.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  maxit: limit on nonlinear iterations\n",
       "\n",
       "  solver: default = \"newton\"\n",
       "\n",
       "  Your choices are \"newton\" or \"chord\". However, you have sham at your\n",
       "  disposal only if you chose newton. \"chord\" will keep using the initial\n",
       "  derivative until the iterate converges, uses the iteration budget, or the\n",
       "  line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "  sham: default = 5 (ie Newton)\n",
       "\n",
       "  This is the Shamanskii method. If sham=1, you have Newton. The iteration\n",
       "  updates the derivative every sham iterations. The convergence rate has local\n",
       "  q-order sham+1 if you only count iterations where you update the derivative.\n",
       "  You need not provide your own derivative function to use this option.\n",
       "  sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "  I made sham=1 the default for scalar equations. For systems I'm more\n",
       "  aggressive and want to invest as little energy in linear algebra as\n",
       "  possible. So the default is sham=5.\n",
       "\n",
       "  armmax: upper bound on step size reductions in line search\n",
       "\n",
       "  resdec: default = .1\n",
       "\n",
       "  This is the target value for residual reduction. The default value is .1. In\n",
       "  the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals\n",
       "  are decreasing rapidly, at least a factor of resdec, and the line search is\n",
       "  quiescent. If you want to eliminate resdec from the method ( you don't )\n",
       "  then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "  armfix: default = false\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the step\n",
       "  size will be fixed at .5. Don't do this unless you are doing experiments for\n",
       "  research.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  If you use pdata in either of F! or J!, you must use in in the calling\n",
       "  sequence of both.\n",
       "\n",
       "  jfact: default = klfact (tries to figure out best choice)\n",
       "\n",
       "  If your Jacobian has any special structure, please set jfact to the correct\n",
       "  choice for a factorization.\n",
       "\n",
       "  I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!)\n",
       "  and factor it. The default is to use klfact (an internal function) to do\n",
       "  something reasonable. For general matrices, klfact picks lu! to compute an\n",
       "  LU factorization and share storage with the Jacobian. You may change LU to\n",
       "  something else by, for example, setting jfact = cholseky! if your Jacobian\n",
       "  is spd.\n",
       "\n",
       "  klfact knows about banded matrices and picks qr. You should, however RTFM,\n",
       "  allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "  If you give me something that klfact does not know how to dispatch on, then\n",
       "  nothing happens. I just return the original Jacobian matrix and nsol will\n",
       "  use backslash to compute the Newton step. I know that this is probably not\n",
       "  optimal in your situation, so it is good to pick something else, like jfact\n",
       "  = lu.\n",
       "\n",
       "  If you want to manage your own factorization within your Jacobian evaluation\n",
       "  function, then set\n",
       "\n",
       "  jfact = nofact\n",
       "\n",
       "  and nsol will not attempt to factor your Jacobian. That is also what happens\n",
       "  when klfact does not know what to do. Your Jacobian is sent directly to\n",
       "  Julia's \\ operation\n",
       "\n",
       "  Please do not mess with the line that calls PrepareJac!.\n",
       "\n",
       "\u001b[36m      FPF = PrepareJac!(FPS, FS, x, ItRules)\u001b[39m\n",
       "\n",
       "  FPF is not the same as FPS (the storage you allocate for the Jacobian) for a\n",
       "  reason. FPF and FPS do not have the same type, even though they share\n",
       "  storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  stagnationok: default = false\n",
       "\n",
       "  Set this to true if you want to disable the line search and either observe\n",
       "  divergence or stagnation. This is only useful for research or writing a\n",
       "  book.\n",
       "\n",
       "  Output:\n",
       "\n",
       "    •  A named tuple (solution, functionval, history, stats, idid,\n",
       "       errcode, solhist)\n",
       "\n",
       "  where\n",
       "\n",
       "  – solution = converged result\n",
       "\n",
       "  – functionval = F(solution)\n",
       "\n",
       "  – history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "  – stats = named tuple of the history of (ifun, ijac, iarm), the number of\n",
       "  functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian evaluation.\n",
       "\n",
       "  – idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  – errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\u001b[36m      = -1 if the initial iterate satisfies the termination criteria\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 10 if no convergence after maxit iterations\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 1  if the line search failed\u001b[39m\n",
       "\n",
       "  – solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iterations + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Examples for nsol\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[1m  World's easiest problem example. Test 64 and 32 bit Jacobians. No\u001b[22m\n",
       "\u001b[1m meaningful difference in the residual histories or the converged\u001b[22m\n",
       "\u001b[1m solutions.\u001b[22m\n",
       "\u001b[1m  --------------------------\u001b[22m\n",
       "\n",
       "\u001b[36m   julia> function f!(fv,x)\u001b[39m\n",
       "\u001b[36m         fv[1]=x[1] + sin(x[2])\u001b[39m\n",
       "\u001b[36m         fv[2]=cos(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         return fv\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  f (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x=ones(2); fv=zeros(2); jv=zeros(2,2); jv32=zeros(Float32,2,2);\u001b[39m\n",
       "\u001b[36m  julia> nout=nsol(f!,x,fv,jv; sham=1);\u001b[39m\n",
       "\u001b[36m  julia> nout32=nsol(f!,x,fv,jv32; sham=1);\u001b[39m\n",
       "\u001b[36m  julia> [nout.history nout32.history]\u001b[39m\n",
       "\u001b[36m  5×2 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00  1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43119e-01  2.43120e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02  1.19231e-02\u001b[39m\n",
       "\u001b[36m   1.03266e-05  1.03265e-05\u001b[39m\n",
       "\u001b[36m   1.46388e-11  1.45995e-11\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [nout.solution nout.solution-nout32.solution]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   -7.39085e-01  -5.48450e-14\u001b[39m\n",
       "\u001b[36m    2.30988e+00  -2.26485e-14\u001b[39m\n",
       "\n",
       "\u001b[1m  H-equation example. I'm taking the sham=5 default here, so the\u001b[22m\n",
       "\u001b[1m convergence is not quadratic. The good news is that we evaluate the\u001b[22m\n",
       "\u001b[1m Jacobian only once.\u001b[22m\n",
       "\u001b[1m  --------------------------\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> n=16; x0=ones(n); FV=ones(n); JV=ones(n,n);\u001b[39m\n",
       "\u001b[36m  julia> hdata=heqinit(x0, .5);\u001b[39m\n",
       "\u001b[36m  julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\u001b[39m\n",
       "\u001b[36m  julia> hout.history\u001b[39m\n",
       "\u001b[36m  4-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   6.17376e-01\u001b[39m\n",
       "\u001b[36m   3.17810e-03\u001b[39m\n",
       "\u001b[36m   2.75227e-05\u001b[39m\n",
       "\u001b[36m   2.35817e-07\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "The calling sequence for the Newton solvers in this book are similar, differing mostly in the management of the linear solver and memory allocation. The calling sequence for __nsol.jl__ is\n",
    "\n",
    "```julia\n",
    "function nsol(\n",
    "    F!,\n",
    "    x0,\n",
    "    FS,\n",
    "    FPS,\n",
    "    J! = diffjac!;\n",
    "    rtol = 1.e-6,\n",
    "    atol = 1.e-12,\n",
    "    maxit = 20,\n",
    "    solver = \"newton\",\n",
    "    sham = 1,\n",
    "    armmax = 10,\n",
    "    resdec = 0.1,\n",
    "    dx = 1.e-7,\n",
    "    armfix = false,\n",
    "    pdata = nothing,\n",
    "    jfact = lu!,\n",
    "    printerr = true,\n",
    "    keepsolhist = false,\n",
    "    stagnationok = false,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier in the chapter, the calling sequence has some new things which are not in __nsolsc.jl__. The most significant are the arrays __FS__ and __FPS__, which preallocate\n",
    "storage for the function and Jacobian. As we have pointed out earlier, the farther upstream one allocates memory the better, so __nsold.jl__ insists that you allocate an vector __FS__ of the same size as the initial iterate and a matrix \n",
    "__FPS__ for the Jacobian. \n",
    "\n",
    "For problems in several variables, the keyword argument __pdata__ is very important. This is the data structure for you to store any precomputed or preallocated data your function evaluation needs. You will almost surely need __pdata__ for any but the most trivial problems. Most of the examples from in this chapter use __pdata__ in a serious manner.\n",
    "\n",
    "You should dimension __x0__ either as $(N)$ (or equivalently $(N,)$) and dimension __FS__ the same way. __nsold.jl__ expects vectors to be in double precision (Float64). Unless you are porting old code you should\n",
    "dimension $\\vx$ and $\\mf$ as vectors $(N)$ not as two dimensional arrays with one column $(N,1)$. The solvers \n",
    "should handle both, but I cannot guarantee that.\n",
    "\n",
    "The __!__ in the function evaluation __F!__ is to indicate that __nsold__ expects __F__  to overwrite its input. So,\n",
    "the way to call __F!__ is to preallocate the storage for the function value in an array __FS__ and then call the function as\n",
    "```Julia\n",
    "F!(FS,x)\n",
    "```\n",
    "or\n",
    "```Julia\n",
    "F!(FS,x,pdata)\n",
    "```\n",
    "__nsold.jl__ will figure out if you have populated __pdata__ or left it alone as the default value of __nothing__.\n",
    "\n",
    "\n",
    "And now for the Jacobian. __nsold.jl__ uses direct methods for linear algebra. If your matrix is dense, the default is to use Julia's __lu!__ function to do an LU factorization. If your matrix is symmetric or symmetric positive definite you can use the __jfact__ keyword to change __lu!__ to __ldlt!__ or __cholesky!__ for example. __nsold.jl__ assumes that the factorization you ask for will overwrite the matrix. Hence, the __factorize__ function in Julia is not what you want for this application.\n",
    "\n",
    "You will also need to preallocate storage for the Jacobian in the array __FPS__. You may use any legal real precision for __FPS__. Float64 is the default. If you use Float32 you cut the storage for the matrix and the time for the factorization in half. We recommend that you do this if your Jacobian is dense. If you are using the __Sparsesuite__ sparse solvers, then you must store the Jacobian in double precision. __Sparsesuite__ does not support lower precision.\n",
    "\n",
    "Your Jacobian computation __J!__ must also overwrite it's input. The call looks like\n",
    "```julia\n",
    "J!(FV,FP,x)\n",
    "```\n",
    "or \n",
    "```julia\n",
    "J!(FV,FP,x,pdata)\n",
    "\n",
    "```\n",
    "returns FP=F'(x). The input FP=F(x), which __nsol.jl__ has already computed, has to be there. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.8.2: H-Equation revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we do several experiments to illustrate the advantages of infrequent evaluation and factorization of the Jacobian and mixed precision computation. We will begin with a function that will support our testing. We want to investigate combinations of Newton's method/Shamanskii with ```sham=5``` (the default), storing and factoring the Jacobian in double/single precision, and analytic/forward difference Jacobians. To make this easy we write a function that solves the H-equation with __nsol.jl__ and lets us vary these cases. \n",
    "\n",
    "The functions for the residual __heqf!.jl__, the Jacobian __heqJ!.jl__, and the precomputed data \n",
    "__heqinit.jl__ are in the large file \n",
    "[src/TestProblems/Systems/Hequation.jl](https://github.com/ctkelley/SIAMFANLEquations.jl/blob/master/src/TestProblems/Systems/Hequation.jl).\n",
    "\n",
    "\n",
    "I'm passing the precomputed data to the function rather than computing it within. This keeps the cost of the precomputed data out of the benchmarking I'll do later.\n",
    "\n",
    "We use __splat__ in this example. We populate a named tuple ```bargs``` to keep the keyword arguments in a convenient place and then, when it's time to give it to __nsol.jl__, the call looks like ```bargs...```. The three dots are the __splat__ and tell __nsol__ to expand bargs and harvest the keyword arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "htest (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function htest(x0, FS, FPS, hdata; analytic=false, hsham=5)\n",
    "    n=length(FS)\n",
    "    #\n",
    "    # I've preallocaed x0, FS, and FPS. But they may have \n",
    "    # been changed by previous runs.\n",
    "    # The cost of resetting their entries to 1.0 is insignificant. \n",
    "    #\n",
    "    FS.=1.0\n",
    "    FPS.=1.0\n",
    "    bargs=(atol = 1.e-10, rtol = 1.e-10, sham = hsham, \n",
    "        resdec = .1, pdata=hdata)\n",
    "    if analytic\n",
    "        nout=nsol( heqf!, x0, FS, FPS, heqJ!; bargs...)\n",
    "    else\n",
    "        nout=nsol( heqf!, x0, FS, FPS; bargs...)\n",
    "    end\n",
    "    return nout\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we will compare the iteration histories for four cases. We will consider analytic and forward difference Jacobians with the storage and factorization of the Jacobian done in double and single precision. __Theorem 1.2__ says the results should be almost indistinguishable. We will begin with Newton's method.\n",
    "\n",
    "All we need to do to store and factor Jacobians is to allocate the storage in single precision. That allocation is the line ```FPS32=ones(Float32,n,n);```. Note that we must reset ```FS``` and ```FPS``` after each call to __nsol.jl__ because the solver uses the storage for residuals and Jacobians for the entire iteration. We do this with __broadcast__ after the initial allocation ```.=1.0``` instead of ```=ones(n,n)``` to avoid reallocation of the Jacobian.\n",
    "\n",
    "We will print all the residual histories in an array. The history vectors are the same length and are very hard to tell apart until the residual norm is one iteration from stagnation. This is just what the theory predicts. The theory \n",
    "(see <cite data-cite=\"ctk:sirev20\"><a href=\"siamfa.html#ctk:sirev20\">(Kel22)</cite> ) also predicts that there will be little difference between double precision linear algebra and single precision. We observe this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float64}:\n",
       " 1.00000e+00  1.00000e+00  1.00000e+00  1.00000e+00\n",
       " 5.14148e-03  5.14131e-03  5.14148e-03  5.14112e-03\n",
       " 1.00479e-07  1.00385e-07  1.00479e-07  1.04305e-07\n",
       " 2.12995e-15  8.11565e-14  2.12995e-15  1.97470e-13"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=1024; FS=ones(n); FPS=ones(n,n); FPS32=ones(Float32,n,n); \n",
    "x0=ones(n); c=.5; hdata = heqinit(x0, c);\n",
    "nouta64=htest(x0, FS, FPS, hdata;\n",
    "    analytic=true, hsham=1);\n",
    "nouta32=htest(x0, FS, FPS32, hdata; \n",
    "    analytic=true, hsham=1);\n",
    "noutfd64=htest(x0, FS, FPS, hdata; \n",
    "    analytic=true, hsham=1);\n",
    "noutfd32=htest(x0, FS, FPS32,hdata; \n",
    "    analytic=false, hsham=1);\n",
    "hinit=nouta64.history[1]\n",
    "ha64=nouta64.history./hinit\n",
    "ha32=nouta32.history./hinit\n",
    "hd64=noutfd64.history./hinit\n",
    "hd32=noutfd32.history./hinit\n",
    "[ha64 ha32 hd64 hd32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do the same thing with the default setting of ```sham=5```. The theory correctly predicts that we will see more nonlinear iterations. We will be using BenchmarkTools to compare the costs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×4 Matrix{Float64}:\n",
       " 1.00000e+00  1.00000e+00  1.00000e+00  1.00000e+00\n",
       " 5.14148e-03  5.14131e-03  5.14126e-03  5.14112e-03\n",
       " 4.44954e-05  4.44938e-05  4.44920e-05  4.44910e-05\n",
       " 3.81019e-07  3.81006e-07  3.80979e-07  3.80968e-07\n",
       " 3.26071e-09  3.26058e-09  3.26027e-09  3.26014e-09\n",
       " 2.79020e-11  2.79000e-11  2.78989e-11  2.78966e-11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouta64=htest(x0, FS, FPS, hdata; analytic=true);\n",
    "noutfd64=htest(x0, FS, FPS, hdata; analytic=false);\n",
    "nouta32=htest(x0, FS, FPS32, hdata; analytic=true);\n",
    "noutfd32=htest(x0, FS, FPS32, hdata; analytic=false);\n",
    "hinit=nouta64.history[1]\n",
    "ha64=nouta64.history./hinit\n",
    "ha32=nouta32.history./hinit\n",
    "hd64=noutfd64.history./hinit\n",
    "hd32=noutfd32.history./hinit\n",
    "[ha64 ha32 hd64 hd32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. We need more iterations, but we evaluate the Jacobian only once for Shamanskii. We can see this by looking at the ```stats``` field of the output tuple. They are all the same, so we will use ```nouta64```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ifun = [1, 1, 1, 1, 1, 1], ijac = [0, 1, 0, 0, 0, 0], iarm = [0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouta64.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation is that we do a function evaluation at all iterations and a single Jacobian evaluation to compute $\\vx_1$. We do no Jacobian work after that. The default in __nsol.jl__ is to reevaluate the Jacobian if the reduction in the residual norm larger than ```resdec = .1```. You can change ```resdec``` in the keyword arguments.\n",
    "The ```iarm``` field in the stats tells us that a line search was not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl) package to look at performance. \n",
    "The ```@btime``` command will show compute time and memory allocations for an average of several runs. The averaging will mitigate the effects of the compile time for the first run. \n",
    "\n",
    "To begin, we will compare the four versions of Newton's method. Note the ```$``` in in fromt of the array arguments. This is __interpolation__ and using it is important if you want to get accurate results from \n",
    "```@btime```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double\n",
      "  11.737 ms (6189 allocations: 468.00 KiB)\n",
      "finite difference, double\n",
      "  75.691 ms (9264 allocations: 731.95 KiB)\n",
      "analytic, single\n",
      "  6.554 ms (6195 allocations: 492.75 KiB)\n",
      "finite difference, single\n",
      "  71.111 ms (9270 allocations: 756.70 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double\"); \n",
    "@btime htest($x0, $FS, $FPS, hdata; \n",
    "    analytic=true, hsham=1);\n",
    "println(\"finite difference, double\"); \n",
    "@btime htest($x0, $FS, $FPS, hdata; \n",
    "    analytic=false, hsham=1);\n",
    "println(\"analytic, single\"); \n",
    "@btime htest($x0, $FS, $FPS32, hdata; \n",
    "    analytic=true, hsham=1);\n",
    "println(\"finite difference, single\"); \n",
    "@btime htest($x0, $FS, $FPS32, hdata; \n",
    "    analytic=false, hsham=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for a problem of only modest size, the differences between the analytic Jacobian and the forward difference are significant. The differences between single and double precision linear algebra are, at least for the analytic Jacobian, roughly the factor of two we'd expect if the matrix factorization dominated the computation. For the forward difference Jacobian, we see that the cost of the Jacobian evaluation dominates everything else.\n",
    "\n",
    "Next, we look at the default ```sham=5``` from __nsol.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double\n",
      "  4.636 ms (2091 allocations: 243.62 KiB)\n",
      "finite difference, double\n",
      "  25.617 ms (3116 allocations: 331.61 KiB)\n",
      "analytic, single\n",
      "  2.584 ms (2101 allocations: 284.88 KiB)\n",
      "finite difference, single\n",
      "  23.696 ms (3126 allocations: 372.86 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double\"); \n",
    "@btime htest($x0, $FS, $FPS, hdata; analytic=true);\n",
    "println(\"finite difference, double\"); \n",
    "@btime htest($x0, $FS, $FPS, hdata; analytic=false);\n",
    "println(\"analytic, single\"); \n",
    "@btime htest($x0, $FS, $FPS32, hdata; analytic=true);\n",
    "println(\"finite difference, single\"); \n",
    "@btime htest($x0, $FS, $FPS32, hdata; analytic=false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral here is pretty clear. We see that compute time is cut by a factor of at least two over Newton's method in all cases. The result for an analytic Jacobian with linear algebra in single precision and ```sham=5``` is 19 times faster than our slowest computation (Newton + finite difference Jacobian + double precision linear algebra). So, do less linear algebra and do it in single precision.\n",
    "\n",
    "Finally, we will increase the dimension. As we do that the computation becomes more burdensome, so we will only do four cases, all with an analytic Jacobian. The size of this example is large enough to clearly show the factor of two reduction in cost one gets from a single precision Jacobian.\n",
    "\n",
    "The reader with lots of free time should try these cases with a forward difference Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double, Newton\n",
      "  402.488 ms (24643 allocations: 1.81 MiB)\n",
      "analytic, single, Newton\n",
      "  194.585 ms (24649 allocations: 1.91 MiB)\n",
      "analytic, double, sham=5\n",
      "  158.527 ms (8253 allocations: 962.22 KiB)\n",
      "analytic, single, sham=5\n",
      "  76.912 ms (8263 allocations: 1.10 MiB)\n"
     ]
    }
   ],
   "source": [
    "n=4096; FS=ones(n); FPS=ones(n,n); \n",
    "FPS32=ones(Float32,n,n); \n",
    "x0=ones(n); c=.5; hdata = heqinit(x0, c);\n",
    "println(\"analytic, double, Newton\"); \n",
    "@btime htest($x0, $FS, $FPS, hdata; \n",
    "    analytic=true, hsham=1);\n",
    "println(\"analytic, single, Newton\"); \n",
    "@btime htest($x0, $FS, $FPS32, hdata; \n",
    "    analytic=true, hsham=1);\n",
    "println(\"analytic, double, sham=5\"); \n",
    "@btime htest($x0, $FS, $FPS, hdata; \n",
    "    analytic=true);\n",
    "println(\"analytic, single, sham=5\"); \n",
    "@btime htest($x0, $FS, $FPS32, hdata; \n",
    "    analytic=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sufficiently large dimension, the linear algebra cost will dominate the computation. Time should increase by roughly a factor of 8 as the dimension doubles because our LU factorization takes $O(N^3)$ operations. We see that with this computation. Increasing the dimension from 1024 to 4096 did increase the runtimes by (roughly) a factor of 64 and we see that using single precision Jacobians cuts the run time in half. Note also that the single precision Shamanskii run is now five times faster than he double precision Newton computation. One of the projects at the end of this chapter challenges you to increase the dimension and compare the timings as you do that. Remember that we allocated storage for the Jacobian when we defined ```FPS```, so @btime is not measuring the allocation for the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8.3: More on the Two Point BVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Jacobian is sparse and you use the solvers from __SuiteSparse__ then you cannot use single precision. Even if the structure of the Jacobian allows you to use the LAPACK solvers or a special-purpose package, there is less benefit in using single precision for linear algebra than in the dense case. We will explore that for the boundary value problem, where we can use [BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) and __qr!__ for the linear solver. __qr!__ supports single precision linear algebra, so we will use that. __lu!__ does not, so the support is not consistent.\n",
    "\n",
    "The we will set up the problem for a very fine mesh, far finer than one needs to get a useful result, to illustrate the performance. The band solver takes $O(N)$ work, so we would expect the solve to be fast. To set things up we mimic __bvp_solve.jl__ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it up\n",
    "    n=10^5;\n",
    "    bdata = bvpinit(n, Float64);\n",
    "#\n",
    "    U0 = zeros(2n);\n",
    "    FV = zeros(2n);\n",
    "# Banded matrix with the correct number of bands\n",
    "# Make double and single precision copies\n",
    "    FPV = BandedMatrix{Float64}(Zeros(2n, 2n), (2, 4));\n",
    "    FPV32 = BandedMatrix{Float32}(Zeros(2n, 2n), (2, 4));\n",
    "#\n",
    "# Build the initial iterate\n",
    "#\n",
    "    tv = bdata.tv;\n",
    "    sv = -.1 * tv .* tv;\n",
    "    view(U0,1:2:2n-1) .= exp.(-.1 .* tv .* tv);\n",
    "    view(U0,2:2:2n).= -.2 .* view(U0,1:2:2n-1) .* tv;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by comparing the default ```sham=5``` with Newton's method. As we did with the H-equation, we will write a test function that uses the data we allocated above. We only use an analytic Jacobian for this and other examples with sparse Jacobians. The reader might want to look at the project in this chapter on [sparse differencing](#Sparse-Differencing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bvptest (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bvptest(U0, FS, FPS, bdata; bsham=5, bfact=qr!)\n",
    "    FS .*= 0.0\n",
    "    FPS .*= 0.\n",
    "        bvpout = nsol(Fbvp!, U0, FS, FPS, Jbvp!; \n",
    "        atol=1.e-8, rtol = 1.e-8, sham=bsham,\n",
    "             pdata = bdata, jfact=bfact)\n",
    "    return bvpout\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bvpoutn64=bvptest(U0, FV, FPV, bdata;  bsham=1);\n",
    "bvpouts64=bvptest(U0, FV, FPV, bdata; bsham=5);\n",
    "bvpoutn32=bvptest(U0, FV, FPV32, bdata;  bsham=1);\n",
    "bvpouts32=bvptest(U0, FV, FPV32, bdata; bsham=5);\n",
    "newtonhist=bvpoutn64.history./bvpoutn64.history[1];\n",
    "shamhist=bvpouts64.history./bvpoutn64.history[1];\n",
    "newtonhist32=bvpoutn32.history./bvpoutn64.history[1];\n",
    "shamhist32=bvpouts32.history./bvpoutn64.history[1];\n",
    "nn=length(newtonhist);\n",
    "ns=length(shamhist);\n",
    "nn32=length(newtonhist32);\n",
    "ns32=length(shamhist32);\n",
    "semilogy(0:nn-1,newtonhist,\"k-\",0:ns-1,shamhist,\"k--\",\n",
    "    0:nn32-1,newtonhist32,\"k.\",0:ns32-1,shamhist32,\"k-.\")\n",
    "legend([\"newton\",\"sham=5\",\"newton, single\",\n",
    "        \"sham=5, single\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 adjoint(::Vector{Int64}) with eltype Int64:\n",
       " 0  0  0  2  2  1  0  0  0  0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bvpoutn64.stats.iarm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the Jacobian less frequently has a smaller impact than in the case of the H-equation. You should expect this because the linear solver costs $O(N)$ work rather than $O(N^3)$ and the Jacobian is updated for the early iterations because the residual is not decreasing fast enough.\n",
    "\n",
    "The other interesting feature of this computation is that the single precision results differ in a meaningful way from the double precision results. The reason for this is that the Jacobian is ill-conditioned enough to affect the quality of the single precision solver. The condition number of the Jacobian is $O(N)$, so for $N=10^5$ one should expect different results from the linear solve in single precision, and that's what you get.\n",
    "\n",
    "Moreover, the benchmark results say that there is no benefit from doing the linear algebra in single precision. This is also no surprise since the factorization is $O(N)$ work.\n",
    "\n",
    "The reader should try this problem with $N=10^6$ and watch the line search fail for the single precision linear algebra computations. That is also no surprise with an inaccurate Newton direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic, double, Newton\n",
      "  151.055 ms (93 allocations: 47.31 MiB)\n",
      "analytic, double, sham=5\n",
      "  125.392 ms (87 allocations: 44.25 MiB)\n",
      "analytic, single, Newton\n",
      "  140.458 ms (129 allocations: 54.17 MiB)\n",
      "analytic, single, sham=5\n",
      "  121.475 ms (131 allocations: 57.23 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"analytic, double, Newton\"); \n",
    "@btime bvptest($U0, $FV, $FPV, $bdata;  bsham=1);\n",
    "println(\"analytic, double, sham=5\"); \n",
    "@btime bvptest($U0, $FV, $FPV, $bdata;  bsham=5);\n",
    "println(\"analytic, single, Newton\"); \n",
    "@btime bvptest($U0, $FV, $FPV32, $bdata;  bsham=1);\n",
    "println(\"analytic, single, sham=5\"); \n",
    "@btime bvptest($U0, $FV, $FPV32, $bdata;  bsham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8.4: Shamanskii for the Convection-Diffusion Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dominant cost in the solution of the PDE problem is the computation and factorization of the sparse Jacobian. Even for the small problem with $31^2 = 961$ unknowns, one can see the effects. We will compare a Newton iteration with the default strategy using ```@btime```. The code that generated the results in section 2.7.5 contained the initialization. It's important to separate that from the solver if you're doing benchmarking. So we will setup the problem first and then benchmark the solver phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton\n",
      "  9.096 ms (539 allocations: 9.51 MiB)\n",
      "sham=5\n",
      "  6.358 ms (382 allocations: 6.55 MiB)\n"
     ]
    }
   ],
   "source": [
    "n=31;\n",
    "# Get some room for the residual\n",
    "u0=zeros(n*n);\n",
    "FV=copy(u0);\n",
    "# Get the precomputed data from pdeinit\n",
    "pdata=pdeinit(n)\n",
    "# Storage for the Jacobian, \n",
    "# same sparsity pattern as the discrete Laplacian\n",
    "J=copy(pdata.D2);\n",
    "println(\"Newton\"); \n",
    "@btime nsol(pdeF!, u0, FV, J, pdeJ!; \n",
    "    resdec=.5, rtol=1.e-7, atol=1.e-10, \n",
    "    pdata=pdata, sham=1);\n",
    "println(\"sham=5\"); \n",
    "@btime nsol(pdeF!, u0, FV, J, pdeJ!; \n",
    "    resdec=.5, rtol=1.e-7, atol=1.e-10, \n",
    "    pdata=pdata, sham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shamanskii iteration, which forms and factors the Jacobian only twice, saves about 40% in time. This is a larger savings that we saw with the two-point boundary value problem, but far from dramatic. The __SuiteSparse__ solvers are very good. The reader should try the same test with a finer mesh to see if the results change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8.5: ptcsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ptcsol.jl__ is our $\\ptc$ solve. As usual, we begin with the docstrings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mi \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mP\u001b[22mar\u001b[0m\u001b[1mt\u001b[22mialQui\u001b[0m\u001b[1mc\u001b[22mk\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12,              maxit=20, delta0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact,                printerr = true, keepsolhist = false, jknowsdt = false)\n",
       "\n",
       "C. T. Kelley, 2022\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsol\n",
       "\n",
       "PTC finds the steady-state solution of u' = -F(u), u(0) = u\\_0. The - sign is a convention.\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is a vector of size N\n",
       "\n",
       "You should store it as (N,) and design F! to use vectors of size (N,). If you use (N,1) consistently instead, the solvers may work, but I make no guarantees.\n",
       "\n",
       "\n",
       "\\item FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "If FPS is sparse, you \\textbf{must} allocate storage for the diagonal so I will have room to put 1/dt in there.\n",
       "\n",
       "\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);   (FP,FV, x) must be the argument list, even if FP does not need FV.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "You may have a better way to add (1/dt) I to your Jacobian. If you   want to do this yourself then your Jacobian function should be   FP=J!(FP,FV,x,dt) or FP=J!(FP,FV,x,dt,pdata) and return   F'(x) + (1.0/dt)*I. \n",
       "\n",
       "You will also have to set the kwarg \\textbf{jknowsdt} to true.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision   functions and linear algebra in any precision you want. You can declare   FPS as Float64, Float32, or Float16 and ptcsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well   conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "delta0: initial pseudo time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to delta0. If your choice of delta0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a straight-up nonlinear solve. This is part of the price for finding the  stable solution.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  ptcsol will use backslash to compute the Newton step.\n",
       "\n",
       "I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "jknowsdt: default = false\n",
       "\n",
       "Set this to true if your Jacobian evaluation function retursn F'(x) + (1/dt) I. You'll also need to follow the rules above for the Jacobian evaluation function. I do not recommend this and if your Jacobian is anything other than a matrix I can't promise anything. I've tested this for matrix outputs only.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "Unlike nsol, nsoli, or even ptcsoli, ptcsol has a fixed cost per  iteration of one function, one Jacobian, and one Factorization. Hence iteration statistics are not interesting and not in the output. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisfies the termination criteria         = 10 if no convergence after maxit iterations\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\subsubsection{Example for ptcsol}\n",
       "\\paragraph{The buckling beam problem.}\n",
       "You'll need to use TestProblems for this to work.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> using SIAMFANLEquations.TestProblems\n",
       "\n",
       "julia> n=63; maxit=1000; delta = 0.01; lambda = 20.0;\n",
       "\n",
       "julia> bdata = beaminit(n, 0.0, lambda); x = bdata.x;\n",
       "\n",
       "julia> u0 = x .* (1.0 .- x) .* (2.0 .- x);\n",
       "\n",
       "julia> u0 .*= exp.(-10.0 * u0);\n",
       "\n",
       "julia> FS = copy(u0); FPS = copy(bdata.D2);\n",
       "\n",
       "julia> pout = ptcsol( FBeam!, u0, FS, FPS, BeamJ!; \n",
       " rtol = 1.e-10, pdata = bdata, delta0 = delta, maxit = maxit);\n",
       "\n",
       "julia> # It takes a few iterations to get there.\n",
       "       length(pout.history)\n",
       "25\n",
       "\n",
       "julia> [pout.history[1:5] pout.history[21:25]]\n",
       "5×2 Array{Float64,2}:\n",
       " 6.31230e+01  9.75412e-01\n",
       " 7.52624e+00  8.35295e-02\n",
       " 8.31545e+00  6.58797e-04\n",
       " 3.15455e+01  4.12697e-08\n",
       " 3.66566e+01  6.29295e-12\n",
       "\n",
       "julia> # We get the nonnegative stedy state.\n",
       "       maximum(pout.solution)\n",
       "2.19086e+00\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12,              maxit=20, delta0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact,                printerr = true, keepsolhist = false, jknowsdt = false)\n",
       "\n",
       "C. T. Kelley, 2022\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsol\n",
       "\n",
       "PTC finds the steady-state solution of u' = -F(u), u(0) = u_0. The - sign is a convention.\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is a vector of size N\n",
       "\n",
       "    You should store it as (N,) and design F! to use vectors of size (N,). If you use (N,1) consistently instead, the solvers may work, but I make no guarantees.\n",
       "  * FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    If FPS is sparse, you **must** allocate storage for the diagonal so I will have room to put 1/dt in there.\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "    So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);   (FP,FV, x) must be the argument list, even if FP does not need FV.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "    You may have a better way to add (1/dt) I to your Jacobian. If you   want to do this yourself then your Jacobian function should be   FP=J!(FP,FV,x,dt) or FP=J!(FP,FV,x,dt,pdata) and return   F'(x) + (1.0/dt)*I. \n",
       "\n",
       "    You will also have to set the kwarg **jknowsdt** to true.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision   functions and linear algebra in any precision you want. You can declare   FPS as Float64, Float32, or Float16 and ptcsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well   conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "    BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "delta0: initial pseudo time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to delta0. If your choice of delta0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a straight-up nonlinear solve. This is part of the price for finding the  stable solution.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-6\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholseky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  ptcsol will use backslash to compute the Newton step.\n",
       "\n",
       "I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "jknowsdt: default = false\n",
       "\n",
       "Set this to true if your Jacobian evaluation function retursn F'(x) + (1/dt) I. You'll also need to follow the rules above for the Jacobian evaluation function. I do not recommend this and if your Jacobian is anything other than a matrix I can't promise anything. I've tested this for matrix outputs only.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "Unlike nsol, nsoli, or even ptcsoli, ptcsol has a fixed cost per  iteration of one function, one Jacobian, and one Factorization. Hence iteration statistics are not interesting and not in the output. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "errcode = 0 if if the iteration succeeded         = -1 if the initial iterate satisfies the termination criteria         = 10 if no convergence after maxit iterations\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "### Example for ptcsol\n",
       "\n",
       "#### The buckling beam problem.\n",
       "\n",
       "You'll need to use TestProblems for this to work.\n",
       "\n",
       "```jldoctest\n",
       "julia> using SIAMFANLEquations.TestProblems\n",
       "\n",
       "julia> n=63; maxit=1000; delta = 0.01; lambda = 20.0;\n",
       "\n",
       "julia> bdata = beaminit(n, 0.0, lambda); x = bdata.x;\n",
       "\n",
       "julia> u0 = x .* (1.0 .- x) .* (2.0 .- x);\n",
       "\n",
       "julia> u0 .*= exp.(-10.0 * u0);\n",
       "\n",
       "julia> FS = copy(u0); FPS = copy(bdata.D2);\n",
       "\n",
       "julia> pout = ptcsol( FBeam!, u0, FS, FPS, BeamJ!; \n",
       " rtol = 1.e-10, pdata = bdata, delta0 = delta, maxit = maxit);\n",
       "\n",
       "julia> # It takes a few iterations to get there.\n",
       "       length(pout.history)\n",
       "25\n",
       "\n",
       "julia> [pout.history[1:5] pout.history[21:25]]\n",
       "5×2 Array{Float64,2}:\n",
       " 6.31230e+01  9.75412e-01\n",
       " 7.52624e+00  8.35295e-02\n",
       " 8.31545e+00  6.58797e-04\n",
       " 3.15455e+01  4.12697e-08\n",
       " 3.66566e+01  6.29295e-12\n",
       "\n",
       "julia> # We get the nonnegative stedy state.\n",
       "       maximum(pout.solution)\n",
       "2.19086e+00\n",
       "```\n"
      ],
      "text/plain": [
       "  ptcsol(F!, x0, FS, FPS, J! = diffjac!; rtol=1.e-6, atol=1.e-12, maxit=20,\n",
       "  delta0=1.e-6, dx=1.e-7, pdata = nothing, jfact = klfact, printerr = true,\n",
       "  keepsolhist = false, jknowsdt = false)\n",
       "\n",
       "  C. T. Kelley, 2022\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: some\n",
       "  new stuff ==> ptcsol\n",
       "\n",
       "  PTC finds the steady-state solution of u' = -F(u), u(0) = u_0. The - sign is\n",
       "  a convention.\n",
       "\n",
       "  You must allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •  F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "       your preallocated storage for the function.\n",
       "       So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "    •  x0: initial iterate\n",
       "\n",
       "    •  FS: Preallocated storage for function. It is a vector of size N\n",
       "       You should store it as (N,) and design F! to use vectors of size\n",
       "       (N,). If you use (N,1) consistently instead, the solvers may work,\n",
       "       but I make no guarantees.\n",
       "\n",
       "    •  FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "       If FPS is sparse, you \u001b[1mmust\u001b[22m allocate storage for the diagonal so I\n",
       "       will have room to put 1/dt in there.\n",
       "\n",
       "    •  J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "       your preallocated storage for the Jacobian. If you leave this out\n",
       "       the default is a finite difference Jacobian.\n",
       "       So, FP=J!(FP,FV,x) or FP=J!(FP,FV,x,pdata) returns FP=F'(x);\n",
       "       (FP,FV, x) must be the argument list, even if FP does not need FV.\n",
       "       One reason for this is that the finite-difference Jacobian does\n",
       "       and that is the default in the solver.\n",
       "       You may have a better way to add (1/dt) I to your Jacobian. If you\n",
       "       want to do this yourself then your Jacobian function should be\n",
       "       FP=J!(FP,FV,x,dt) or FP=J!(FP,FV,x,dt,pdata) and return F'(x) +\n",
       "       (1.0/dt)*I.\n",
       "       You will also have to set the kwarg \u001b[1mjknowsdt\u001b[22m to true.\n",
       "\n",
       "    •  Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "       full precision functions and linear algebra in any precision you\n",
       "       want. You can declare FPS as Float64, Float32, or Float16 and\n",
       "       ptcsol will do the right thing if YOU do not destroy the\n",
       "       declaration in your J! function. I'm amazed that this works so\n",
       "       easily. If the Jacobian is reasonably well conditioned, you can\n",
       "       cut the cost of Jacobian factorization and storage in half with no\n",
       "       loss. For large dense Jacobians and inexpensive functions, this is\n",
       "       a good deal.\n",
       "       BUT ... There is very limited support for direct sparse solvers in\n",
       "       anything other than Float64. I recommend that you only use Float64\n",
       "       with direct sparse solvers unless you really know what you're\n",
       "       doing. I have a couple examples in the notebook, but watch out.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  delta0: initial pseudo time step. The default value of 1.e-3 is a bit\n",
       "  conservative and is one option you really should play with. Look at the\n",
       "  example where I set it to 1.0!\n",
       "\n",
       "  maxit: limit on nonlinear iterations, default=100.\n",
       "\n",
       "  This is coupled to delta0. If your choice of delta0 is too small\n",
       "  (conservative) then you'll need many iterations to converge and will need a\n",
       "  larger value of maxit\n",
       "\n",
       "  For PTC you'll need more iterations than for a straight-up nonlinear solve.\n",
       "  This is part of the price for finding the stable solution.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x)+1.e-6\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  jfact: default = klfact (tries to figure out best choice)\n",
       "\n",
       "  If your Jacobian has any special structure, please set jfact to the correct\n",
       "  choice for a factorization.\n",
       "\n",
       "  I use jfact when I call PTCUpdate to evaluate the Jacobian (using your J!)\n",
       "  and factor it. The default is to use klfact (an internal function) to do\n",
       "  something reasonable. For general matrices, klfact picks lu! to compute an\n",
       "  LU factorization and share storage with the Jacobian. You may change LU to\n",
       "  something else by, for example, setting jfact = cholseky! if your Jacobian\n",
       "  is spd.\n",
       "\n",
       "  klfact knows about banded matrices and picks qr. You should, however RTFM,\n",
       "  allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "  If you give me something that klfact does not know how to dispatch on, then\n",
       "  nothing happens. I just return the original Jacobian matrix and ptcsol will\n",
       "  use backslash to compute the Newton step.\n",
       "\n",
       "  I know that this is probably not optimal in your situation, so it is good to\n",
       "  pick something else, like jfact = lu.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  jknowsdt: default = false\n",
       "\n",
       "  Set this to true if your Jacobian evaluation function retursn F'(x) + (1/dt)\n",
       "  I. You'll also need to follow the rules above for the Jacobian evaluation\n",
       "  function. I do not recommend this and if your Jacobian is anything other\n",
       "  than a matrix I can't promise anything. I've tested this for matrix outputs\n",
       "  only.\n",
       "\n",
       "  Output:\n",
       "\n",
       "  A named tuple (solution, functionval, history, stats, idid, errcode,\n",
       "  solhist) where\n",
       "\n",
       "  solution = converged result functionval = F(solution) history = the vector\n",
       "  of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "  Unlike nsol, nsoli, or even ptcsoli, ptcsol has a fixed cost per iteration\n",
       "  of one function, one Jacobian, and one Factorization. Hence iteration\n",
       "  statistics are not interesting and not in the output.\n",
       "\n",
       "  idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  errcode = 0 if if the iteration succeeded = -1 if the initial iterate\n",
       "  satisfies the termination criteria = 10 if no convergence after maxit\n",
       "  iterations\n",
       "\n",
       "  solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\u001b[1m  Example for ptcsol\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[1m  The buckling beam problem.\u001b[22m\n",
       "\u001b[1m  ----------------------------\u001b[22m\n",
       "\n",
       "  You'll need to use TestProblems for this to work.\n",
       "\n",
       "\u001b[36m  julia> using SIAMFANLEquations.TestProblems\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> n=63; maxit=1000; delta = 0.01; lambda = 20.0;\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> bdata = beaminit(n, 0.0, lambda); x = bdata.x;\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> u0 = x .* (1.0 .- x) .* (2.0 .- x);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> u0 .*= exp.(-10.0 * u0);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> FS = copy(u0); FPS = copy(bdata.D2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> pout = ptcsol( FBeam!, u0, FS, FPS, BeamJ!; \u001b[39m\n",
       "\u001b[36m   rtol = 1.e-10, pdata = bdata, delta0 = delta, maxit = maxit);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # It takes a few iterations to get there.\u001b[39m\n",
       "\u001b[36m         length(pout.history)\u001b[39m\n",
       "\u001b[36m  25\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [pout.history[1:5] pout.history[21:25]]\u001b[39m\n",
       "\u001b[36m  5×2 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   6.31230e+01  9.75412e-01\u001b[39m\n",
       "\u001b[36m   7.52624e+00  8.35295e-02\u001b[39m\n",
       "\u001b[36m   8.31545e+00  6.58797e-04\u001b[39m\n",
       "\u001b[36m   3.15455e+01  4.12697e-08\u001b[39m\n",
       "\u001b[36m   3.66566e+01  6.29295e-12\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # We get the nonnegative stedy state.\u001b[39m\n",
       "\u001b[36m         maximum(pout.solution)\u001b[39m\n",
       "\u001b[36m  2.19086e+00\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ptcsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the Buckling Beam\n",
    "\n",
    "The residual function for the beam uses precomputed data to store the discrete second derivative and the bifurcation parameter $\\lambda$. These data are smaller and far less complex that the data for the H-equation and it's worthwhile too look at the functions. These are part of the __TestProblems__ submodule for __SIAMFANLEquations.jl__. We will not conver all the details here, but will give you enough to see how precomputed data is useful. The file is __TestProblems/Systems/FBeam!.jl__.\n",
    "\n",
    "First the function itself\n",
    "```Julia\n",
    "\"\"\"\n",
    "FBeam!(FV, U, bdata)\n",
    "Function evaluation for PTC example.\n",
    "F(u) = -u'' - lambda sin(u)\n",
    "\"\"\"\n",
    "function FBeam!(FV, U, bdata)\n",
    "    D2 = bdata.D2\n",
    "    lambda = bdata.lambda\n",
    "    su = lambda * sin.(U)\n",
    "    FV .= (D2 * U - su)\n",
    "end\n",
    "```\n",
    "Note how the function ```FBeam``` harvests $\\lambda$ and the discrete second derivative ```D2``` from the precomputed data ```bdata```. In this case ```bdata``` is a __named tuple__ which we create with the\n",
    "__beaminit__ function.\n",
    "\n",
    "```Julia\n",
    "\"\"\"\n",
    "beaminit(n,dt,lambda=20.0)\n",
    "\n",
    "Set up the beam problem with n interior grid points.\n",
    "\"\"\"\n",
    "\n",
    "function beaminit(n, dt, lambda = 20.0)\n",
    "    D2 = Lap1d(n)\n",
    "    dx = 1.0 / (n + 1)\n",
    "    x = collect(dx:dx:1.0-dx)\n",
    "    UN = zeros(size(x))\n",
    "    bdata = (D2 = D2, x = x, dx = dx, dt = dt, \n",
    "             lambda = lambda, UN = UN)\n",
    "end\n",
    "```\n",
    "The bdata structure has more that just ```D2``` and $\\lambda$. I data for the solver and the construction of the Jacobian.\n",
    "\n",
    "Finally, the function __Lap1D__ computes ```D2``` as a tridiagonal matrix. This is part of the LinearAlgebra package that is part of Julia.Base.\n",
    "\n",
    "```Julia\n",
    "\"\"\"\n",
    "Lap1d(n)\n",
    "\n",
    "returns -d^2/dx^2 on [0,1] zero BC\n",
    "\"\"\"\n",
    "function Lap1d(n)\n",
    "    dx = 1 / (n + 1)\n",
    "    d = 2.0 * ones(n)\n",
    "    sup = -ones(n - 1)\n",
    "    slo = -ones(n - 1)\n",
    "    D2 = Tridiagonal(slo, d, sup)\n",
    "    D2 = D2 / (dx * dx)\n",
    "    return D2\n",
    "end\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.9 Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on the H-equation\n",
    "\n",
    "- Run the H-equation tests with $c=.99$ and larger dimensions ($2^k$ for $k=12, 13, 14$). Are the values of ```sham``` and ```resdec``` optimal for this case? How, for example, is ```resdec = Inf``` (your author's favorite value). To see why I like ```sham=Inf``` have a look at\n",
    "<cite data-cite=\"brent\"><a href=\"siamfa.html#brent\">(Bre73)</cite>.\n",
    "\n",
    "- What happens with $c=1$? Is the chord method a good idea? \n",
    "Don't look at\n",
    "<cite data-cite=\"ctk:sirev20\"><a href=\"siamfa.html#ctk:sirev20\">(Kel22)</cite>,\n",
    "<cite data-cite=\"ctk:n1\"><a href=\"siamfa.html#ctk:n1\">(DK80)</cite>, or\n",
    "<cite data-cite=\"ctk:chord\"><a href=\"siamfa.html#ctk:chord\">(DK83)</cite> before trying to figure things out on your own.\n",
    "\n",
    "- Increase the dimension as much as you can and compare single and double precision linear algebra.\n",
    "    \n",
    "- Duplicate the table on page 125 of <cite data-cite=\"chand\"><a href=\"siamfa.html#chand\">(Cha60)</cite>.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Not trivial!)__ Try using __automatic differention__ to compute the Jacobian for the examples in this chapter and use the results to write a Jacobian evaluation function to pass to __nsol.jl__.\n",
    "You will first have to decide how you want to do that. Two mature packages are [ForwardDiff.jl](#https://github.com/JuliaDiff/ForwardDiff.jl)\n",
    "<cite data-cite=\"forwarddiff\"><a href=\"siamfa.html#forwarddiff\">(RPL16)</cite>\n",
    "and [Zygote.jl](#https://github.com/FluxML/Zygote.jl) \n",
    "<cite data-cite=\"zygote\"><a href=\"siamfa.html#zygote\">(CoRR)</cite>.\n",
    "Books like\n",
    "<cite data-cite=\"grautodiff\"><a href=\"siamfa.html#grautodiff\">(Gri00)</cite> explain the\n",
    "algorithmic differences between these packages.\n",
    "    \n",
    "How does the performance compare to an analytic or forward difference Jacobian? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving a differential or integral equation\n",
    "by __nested iteration__ or __grid sequencing__ means resolving\n",
    "the rough features of the solution of a differential or\n",
    "integral equation on a coarse mesh, interpolating the\n",
    "solution to a finer mesh, resolving on the finer mesh, and\n",
    "then repeating the process until you have a solution on a target\n",
    "mesh.\n",
    "\n",
    "Apply this idea to some of the examples in the text, using piecewise\n",
    "linear interpolation to move from coarse to fine meshes. If the\n",
    "discretization is second-order accurate and you halve the mesh\n",
    "width at each level, how should you terminate the solver at\n",
    "each level? What kind of iteration statistics would tell you that\n",
    "you've done a satisfactory job?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Jacobian evaluation functions using sparse differencing for the boundary value problem and the buckling beam examples. You can try, for example, the methods from \n",
    "<cite data-cite=\"curtispr\"><a href=\"siamfa.html#curtisor\">(CPR74)</cite> or\n",
    "<cite data-cite=\"colmore\"><a href=\"siamfa.html#colmore\">(CM83)</cite>.  <cite data-cite=\"ctk:newton\"><a href=\"siamfa.html#ctk:newton\">(Kel03)</cite> has a Matlab code for banded matrices that, while convertible to Julia, needs some work to explictly store the Jacobian as a BandedMatrix so\n",
    "[BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) will solve the linear system efficiently.\n",
    "Another alternative is to use __sparsefdiff__ \n",
    "<cite data-cite=\"sparsediff\"><a href=\"siamfa.html#sparsediff\">(RMGH20)</cite> from\n",
    "[SparseDiffTools.jl](#https://github.com/JuliaDiff/SparseDiffTools.jl). \n",
    "    \n",
    "No matter what you do, you'll need to think about fill-in, symbolic factorization in the general case, and storage. \n",
    "    \n",
    "Have fun!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ```lu!``` for Sparse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is for Julia 1.5 and higher. The option ```jfact=nofact``` in __nsol.jl__ is there so you can build and manage your own factorization inside your Jacobian evaluation function. Use this feature to store a factorization of the Jacobian for the PDE problem and reuse that storage rather than reallocating with every call to lu. What are the savings in time and allocations? You'll need to figure out how to use ```lu!``` for sparse matrices to do this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does $\\delta_0$ depend on $\\delta_x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary $\\delta_x$ and $\\delta_0$ in the buckling beam example. For \n",
    "the fixed $\\delta_x = 1/64$ in the example, explore the effects of\n",
    "increasing $\\delta_0$. Can you converge to the ``wrong'' (\\ie negative)\n",
    "solution in this way? What happens if you fix $\\delta_0$ and reduce\n",
    "$\\delta_x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next notebook = [Chapter 3: Newton-Krylov Methods](SIAMFANLCh3.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
