{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.8 Solvers for Chapter 2\n",
    "\n",
    "Contents for Section 2.8\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "- [nsol.jl](#Section-2.8.1:-nsol.jl)\n",
    "\n",
    "- [H-equation Revisited](#Section-2.8.2:-H-Equation-revisited)\n",
    "\n",
    "- [More on the Two-Point BVP](#Section-2.8.3:-More-on-the-Two-Point-BVP)\n",
    "\n",
    "- [Shamanskii for the Convection-Diffusion Problem](#Section-2.8.4:-Shamanskii-for-the-Convection-Diffusion-Problem)\n",
    "\n",
    "- [ptcsol.jl](#Section-2.8.5:-ptcsol.jl)\n",
    "\n",
    "- [More on the Buckling Beam](#Benchmarking-the-Buckling-Beam)\n",
    "\n",
    "## [Section 2.9: Projects](#Section-2.9-Projects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the pattern of Chapter 1 and present two solvers, a Newton code and a $\\ptc$ code. Both codes are for systems of equations and use direct methods to compute the step. We returned the solution history for the simple two dimensional example in Section 2.6, but will not do that again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8.1: nsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsol.jl__ solves systems of nonlinear equations and computes the Newton step with direct linear solvers. Let's look at the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "?nsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "\n",
    "The calling sequence for the Newton solvers in this book are similar, differing mostly in the management of the linear solver and memory allocation. The calling sequence for __nsol.jl__ is\n",
    "\n",
    "```julia\n",
    "function nsol(\n",
    "    F!,\n",
    "    x0,\n",
    "    FS,\n",
    "    FPS,\n",
    "    J! = diffjac!;\n",
    "    rtol = 1.e-6,\n",
    "    atol = 1.e-12,\n",
    "    maxit = 20,\n",
    "    solver = \"newton\",\n",
    "    sham = 1,\n",
    "    armmax = 10,\n",
    "    resdec = 0.1,\n",
    "    dx = 1.e-7,\n",
    "    armfix = false,\n",
    "    pdata = nothing,\n",
    "    jfact = lu!,\n",
    "    printerr = true,\n",
    "    keepsolhist = false,\n",
    "    stagnationok = false,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier in the chapter, the calling sequence has some new things which are not in __nsolsc.jl__. The most significant are the arrays __FS__ and __FPS__, which preallocate\n",
    "storage for the function and Jacobian. As we have pointed out earlier, the farther upstream one allocates memory the better, so __nsold.jl__ insists that you allocate an vector __FS__ of the same size as the initial iterate and a matrix \n",
    "__FPS__ for the Jacobian. \n",
    "\n",
    "The other major new feature is the keyword argument __pdata__. This is the data structure for you to store any precomputed or preallocated data your function evaluation needs. You will almost surely need __pdata__ for any but the most trivial problems. The H-equation example from section 2.6 uses __pdata__ in a serious manner.\n",
    "\n",
    "You should dimension __x0__ either as $(N,)$ and dimension __FS__ the same way. __nsold.jl__ expects vectors to be in double precision (Float64).\n",
    "\n",
    "The __!__ in the function evaluation __F!__ is to indicate that __nsold__ expects __F__  to overwrite its input. So,\n",
    "the way to call __F!__ is to preallocate the storage for the function value in an array __FS__ and then call the function as\n",
    "```Julia\n",
    "F!(FS,x)\n",
    "```\n",
    "or\n",
    "```Julia\n",
    "F!(FS,x,pdata)\n",
    "```\n",
    "__nsold.jl__ will figure out if you have populated __pdata__ or left it alone as the default value of __nothing__.\n",
    "\n",
    "\n",
    "And now for the Jacobian. __nsold.jl__ uses direct methods for linear algebra. If your matrix is dense, the default is to use Julia's __lu!__ function to do an LU factorization. If your matrix is symmetric or symmetric positive definite you can use the __factorization__ keyword to change __lu!__ to __ldlt!__ or __cholesky!__ for example. __nsold.jl__ assumes that the factorization you ask for will overwrite the matrix. Hence, the __factorize__ function in Julia is not what you want for this application.\n",
    "\n",
    "You will also need to preallocate storage for the Jacobian in the array __FPS__. You may use any legal real precision for __FPS__. Float64 is the default. If you use Float32 you cut the storage for the matrix and the time for the factorization in half. We recommend that you do this if your Jacobian is dense. If you are using the __Sparsesuite__ sparse solvers, then you must store the Jacobian in double precision. __Sparsesuite__ does not support lower precision.\n",
    "\n",
    "Your Jacobian computation __J!__ must also overwrite it's input. The call looks like\n",
    "```julia\n",
    "J!(FV,FP,x)\n",
    "```\n",
    "or \n",
    "```julia\n",
    "J!(FV,FP,x,pdata)\n",
    "\n",
    "```\n",
    "returns FP=F'(x). The input FP=F(x), which __nsol.jl__ has already computed, has to be there. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.8.2: H-Equation revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we do several experiments to illustrate the advantages of infrequent evaluation and factorization of the Jacobian and mixed precision computation. We will begin with a function that will support our testing. We want to investigate combinations of Newton's method/Shamanskii with ```sham=5``` (the default), storing and factoring the Jacobian in double/single precision, and analytic/forward difference Jacobians. To make this easy we write a function that solves the H-equation with __nsol.jl__ and lets us vary these cases. \n",
    "\n",
    "The functions for the residual __heqf!.jl__, the Jacobian __heqJ!.jl__, and the precomputed data \n",
    "__heqinit.jl__ are in the large file \n",
    "[src/TestProblems/Systems/Hequation.jl](https://github.com/ctkelley/SIAMFANLEquations.jl/blob/master/src/TestProblems/Systems/Hequation.jl).\n",
    "\n",
    "\n",
    "I'm passing the precomputed data to the function rather than computing it within. This keeps the cost of the precomputed data out of the benchmarking I'll do later.\n",
    "\n",
    "We use __splat__ in this example. We populate a named tuple ```bargs``` to keep the keyword arguments in a convenient place and then, when it's time to give it to __nsol.jl__, the call looks like ```bargs...```. The three dots are the __splat__ and tell __nsol__ to expand bargs and harvest the keyword arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function htest(x0, FS, FPS, hdata; analytic=false, hsham=5)\n",
    "    n=length(FS)\n",
    "    #\n",
    "    # I've preallocaed x0, FS, and FPS. But they may have been changed by previous runs.\n",
    "    # The cost of resetting their entries to 1.0 is insignificant. \n",
    "    #\n",
    "    FS.=1.0\n",
    "    FPS.=1.0\n",
    "    bargs=(atol = 1.e-10, rtol = 1.e-10, sham = hsham, resdec = .1, pdata=hdata)\n",
    "    if analytic\n",
    "        nout=nsol( heqf!, x0, FS, FPS, heqJ!; bargs...)\n",
    "    else\n",
    "        nout=nsol( heqf!, x0, FS, FPS; bargs...)\n",
    "    end\n",
    "    return nout\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we will compare the iteration histories for four cases. We will consider analytic and forward difference Jacobians with the storage and factorization of the Jacobian done in double and single precision. __Theorem 1.2__ says the results should be almost indistinguishable. We will begin with Newton's method.\n",
    "\n",
    "All we need to do to store and factor Jacobians is to allocate the storage in single precision. That allocation is the line ```FPS32=ones(Float32,n,n);```. Note that we must reset ```FS``` and ```FPS``` after each call to __nsol.jl__ because the solver uses the storage for residuals and Jacobians for the entire iteration. We do this with __broadcast__ after the initial allocation ```.=1.0``` instead of ```=ones(n,n)``` to avoid reallocation of the Jacobian.\n",
    "\n",
    "We will print all the residual histories in an array. The history vectors are the same length and are very hard to tell apart until the residual norm is one iteration from stagnation. This is just what the theory predicts. The theory \n",
    "(see <cite data-cite=\"ctk:sirev20\"><a href=\"siamfa.html#ctk:sirev20\">(Kel20a)</cite> ) also predicts that there will be little difference between double precision linear algebra and single precision. We observe this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=1024; FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "nouta64=htest(x0, FS, FPS, hdata;analytic=true, hsham=1);\n",
    "nouta32=htest(x0, FS, FPS32, hdata; analytic=true, hsham=1);\n",
    "noutfd64=htest(x0, FS, FPS, hdata; analytic=true, hsham=1);\n",
    "noutfd32=htest(x0, FS, FPS32,hdata; analytic=false, hsham=1);\n",
    "[nouta64.history nouta32.history noutfd64.history noutfd32.history]./nouta64.history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do the same thing with the default setting of ```sham=5```. The theory correctly predicts that we will see more nonlinear iterations. We will be using BenchmarkTools to compare the costs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouta64=htest(x0, FS, FPS, hdata; analytic=true);\n",
    "noutfd64=htest(x0, FS, FPS, hdata; analytic=false);\n",
    "nouta32=htest(x0, FS, FPS32, hdata; analytic=true);\n",
    "noutfd32=htest(x0, FS, FPS32, hdata; analytic=false);\n",
    "[nouta64.history nouta32.history noutfd64.history noutfd32.history]./nouta64.history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. We need more iterations, but we evaluate the Jacobian only once for Shamanskii. We can see this by looking at the ```stats``` field of the output tuple. They are all the same, so we will use ```nouta64```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouta64.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation is that we do a function evaluation at all iterations and a single Jacobian evaluation to compute $\\vx_1$. We do no Jacobian work after that. The default in __nsol.jl__ is to reevaluate the Jacobian if the reduction in the residual norm larger than ```resdec = .1```. You can change ```resdec``` in the keyword arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl) package to look at performance. \n",
    "The ```@btime``` command will show compute time and memory allocations for an average of several runs. The averaging will mitigate the effects of the compile time for the first run. \n",
    "\n",
    "To begin, we will compare the four versions of Newton's method. Note the ```$``` in in fromt of the array arguments. This is __interpolation__ and using it is important if you want to get accurate results from \n",
    "```@btime```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "println(\"analytic, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true, hsham=1);\n",
    "println(\"finite difference, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=false, hsham=1);\n",
    "println(\"analytic, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true, hsham=1);\n",
    "println(\"finite difference, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=false, hsham=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for a problem of only modest size, the differences between the analytic Jacobian and the forward difference are significant. The differences between single and double precision linear algebra are, at least for the analytic Jacobian, roughly the factor of two we'd expect if the matrix factorization dominated the computation. For the forward difference Jacobian, we see that the cost of the Jacobian evaluation dominates everything else.\n",
    "\n",
    "Next, we look at the default ```sham=5``` from __nsol.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "println(\"analytic, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true);\n",
    "println(\"finite difference, double\"); @btime htest($x0, $FS, $FPS, hdata; analytic=false);\n",
    "println(\"analytic, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true);\n",
    "println(\"finite difference, single\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral here is pretty clear. We see that compute time is cut by a factor of at least two over Newton's method in all cases. The result for an analytic Jacobian with linear algebra in single precision and ```sham=5``` is 19 times faster than our slowest computation (Newton + finite difference Jacobian + double precision linear algebra). So, do less linear algebra and do it in single precision.\n",
    "\n",
    "Finally, we will increase the dimension. As we do that the computation becomes more burdensome, so we will only do four cases, all with an analytic Jacobian. The size of this example is large enough to clearly show the factor of two reduction in cost one gets from a single precision Jacobian.\n",
    "\n",
    "The reader with lots of free time should try these cases with a forward difference Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=4096; FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "println(\"analytic, double, Newton\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true, hsham=1);\n",
    "println(\"analytic, single, Newton\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true, hsham=1);\n",
    "println(\"analytic, double, sham=5\"); @btime htest($x0, $FS, $FPS, hdata; analytic=true);\n",
    "println(\"analytic, single, sham=5\"); @btime htest($x0, $FS, $FPS32, hdata; analytic=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sufficiently large dimension, the linear algebra cost will dominate the computation. Time should increase by roughly a factor of 8 as the dimension doubles because our LU factorization takes $O(N^3)$ operations. We see that with this computation. Increasing the dimension from 1024 to 4096 did increase the runtimes by (roughly) a factor of 64 and we see that using single precision Jacobians cuts the run time in half. Note also that the single precision Shamanskii run is now five times faster than he double precision Newton computation. One of the projects at the end of this chapter challenges you to increase the dimension and compare the timings as you do that. Remember that we allocated storage for the Jacobian when we defined ```FPS```, so @btime is not measuring the allocation for the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8.3: More on the Two Point BVP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Jacobian is sparse and you use the solvers from __SuiteSparse__ then you cannot use single precision. Even if the structure of the Jacobian allows you to use the LAPACK solvers or a special-purpose package, there is less benefit in using single precision for linear algebra than in the dense case. We will explore that for the boundary value problem, where we can use [BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) and __qr!__ for the linear solver. __qr!__ supports single precision linear algebra, so we will use that. __lu!__ does not, so the support is not consistent.\n",
    "\n",
    "The we will set up the problem for a very fine mesh, far finer than one needs to get a useful result, to illustrate the performance. The band solver takes $O(N)$ work, so we would expect the solve to be fast. To set things up we mimic __bvp_solve.jl__ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it up\n",
    "    n=10^5;\n",
    "    bdata = bvpinit(n, Float64);\n",
    "#\n",
    "    U0 = zeros(2n);\n",
    "    FV = zeros(2n);\n",
    "# Banded matrix with the correct number of bands\n",
    "# Make double and single precision copies\n",
    "    FPV = BandedMatrix{Float64}(Zeros(2n, 2n), (2, 4));\n",
    "    FPV32 = BandedMatrix{Float32}(Zeros(2n, 2n), (2, 4));\n",
    "#\n",
    "# Build the initial iterate\n",
    "#\n",
    "    tv = bdata.tv;\n",
    "    sv = -.1 * tv .* tv;\n",
    "    view(U0,1:2:2n-1) .= exp.(-.1 .* tv .* tv);\n",
    "    view(U0,2:2:2n).= -.2 .* view(U0,1:2:2n-1) .* tv;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by comparing the default ```sham=5``` with Newton's method. As we did with the H-equation, we will write a test function that uses the data we allocated above. We only use an analytic Jacobian for this and other examples with sparse Jacobians. The reader might want to look at the project in this chapter on [sparse differencing](#Sparse-Differencing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function bvptest(U0, FS, FPS, bdata; bsham=5, bfact=qr!)\n",
    "    FS .*= 0.0\n",
    "    FPS .*= 0.\n",
    "        bvpout = nsol(Fbvp!, U0, FS, FPS, Jbvp!; atol=1.e-8, rtol = 1.e-8, sham=bsham,\n",
    "             pdata = bdata, jfact=bfact)\n",
    "    return bvpout\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvpoutn64=bvptest(U0, FV, FPV, bdata;  bsham=1);\n",
    "bvpouts64=bvptest(U0, FV, FPV, bdata; bsham=5);\n",
    "bvpoutn32=bvptest(U0, FV, FPV32, bdata;  bsham=1);\n",
    "bvpouts32=bvptest(U0, FV, FPV32, bdata; bsham=5);\n",
    "newtonhist=bvpoutn64.history./bvpoutn64.history[1];\n",
    "shamhist=bvpouts64.history./bvpoutn64.history[1];\n",
    "newtonhist32=bvpoutn32.history./bvpoutn64.history[1];\n",
    "shamhist32=bvpouts32.history./bvpoutn64.history[1];\n",
    "nn=length(newtonhist);\n",
    "ns=length(shamhist);\n",
    "nn32=length(newtonhist32);\n",
    "ns32=length(shamhist32);\n",
    "semilogy(0:nn-1,newtonhist,\"k-\",0:ns-1,shamhist,\"k--\",0:nn32-1,newtonhist32,\"k.\",0:ns32-1,shamhist32,\"k-.\")\n",
    "legend([\"newton\",\"sham=5\",\"newton, single\",\"sham=5, single\"]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvpoutn64.stats.iarm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the Jacobian less frequently has a smaller impact than in the case of the H-equation. You should expect this because the linear solver costs $O(N)$ work rather than $O(N^3)$ and the Jacobian is updated for the early iterations because the residual is not decreasing fast enough.\n",
    "\n",
    "The other interesting feature of this computation is that the single precision results differ in a meaningful way from the double precision results. The reason for this is that the Jacobian is ill-conditioned enough to affect the quality of the single precision solver. The condition number of the Jacobian is $O(1/N)$, so for $N=10^5$ one should expect poor results from the linear solve in single precision, and that's what you get.\n",
    "\n",
    "Moreover, the benchmark results say that there is no benefit from doing the linear algebra in single precision. This is also no surprise since the factorization is $O(N)$ work.\n",
    "\n",
    "The reader should try this problem with $N=10^6$ and watch the line search fail for the single precision linear algebra computations. That is also no surprise with an inaccurate Newton direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"analytic, double, Newton\"); @btime bvptest($U0, $FV, $FPV, $bdata;  bsham=1);\n",
    "println(\"analytic, double, sham=5\"); @btime bvptest($U0, $FV, $FPV, $bdata;  bsham=5);\n",
    "println(\"analytic, single, Newton\"); @btime bvptest($U0, $FV, $FPV32, $bdata;  bsham=1);\n",
    "println(\"analytic, single, sham=5\"); @btime bvptest($U0, $FV, $FPV32, $bdata;  bsham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8.4: Shamanskii for the Convection-Diffusion Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dominant cost in the solution of the PDE problem is the computation and factorization of the sparse Jacobian. Even for the small problem with $31^2 = 961$ unknowns, one can see the effects. We will compare a Newton iteration with the default strategy using __btime__. The code that generated the results in section 2.7.5 contained the initialization. It's important to separate that from the solver if you're doing benchmarking. So we will setup the problem first and then benchmark the solver phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=31;\n",
    "# Get some room for the residual\n",
    "u0=zeros(n*n,);\n",
    "FV=copy(u0);\n",
    "# Get the precomputed data from pdeinit\n",
    "pdata=pdeinit(n)\n",
    "# Storage for the Jacobian, same sparsity pattern as the discrete Laplacian\n",
    "J=copy(pdata.D2);\n",
    "println(\"Newton\"); @btime nsol(pdeF!, u0, FV, J, pdeJ!; resdec=.5, rtol=1.e-7, atol=1.e-10, pdata=pdata, sham=1);\n",
    "println(\"sham=5\"); @btime nsol(pdeF!, u0, FV, J, pdeJ!; resdec=.5, rtol=1.e-7, atol=1.e-10, pdata=pdata, sham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shamanskii iteration, which forms and factors the Jacobian only twice, saves about 40% in time. This is a larger savings that we saw with the two-point boundary value problem, but far from dramatic. The __SuiteSparse__ solvers are very good. The reader should try the same test with a finer mesh to see if the results change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.8.5: ptcsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ptcsol.jl__ is our $\\ptc$ solve. As usual, we begin with the docstrings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "?ptcsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the Buckling Beam\n",
    "\n",
    "The residual function for the beam uses precomputed data to store the discrete second derivative and the bifurcation parameter $\\lambda$. These data are smaller and far less complex that the data for the H-equation and it's worthwhile too look at the functions. These are part of the __TestProblems__ submodule for __SIAMFANLEquations.jl__. We will not conver all the details here, but will give you enough to see how precomputed data is useful.\n",
    "\n",
    "First the function itself\n",
    "```Julia\n",
    "\"\"\"\n",
    "FBeam!(FV, U, bdata)\n",
    "Function evaluation for PTC example.\n",
    "F(u) = -u'' - lambda sin(u)\n",
    "\"\"\"\n",
    "function FBeam!(FV, U, bdata)\n",
    "    D2 = bdata.D2\n",
    "    lambda = bdata.lambda\n",
    "    su = lambda * sin.(U)\n",
    "    FV .= (D2 * U - su)\n",
    "end\n",
    "```\n",
    "Note how the function ```FBeam``` harvests $\\lambda$ and the discrete second derivative ```D2``` from the precomputed data ```bdata```. In this case ```bdata``` is a __named tuple__ which we create with the\n",
    "__beaminit__ function.\n",
    "\n",
    "```Julia\n",
    "\"\"\"\n",
    "beaminit(n,dt,lambda=20.0)\n",
    "\n",
    "Set up the beam problem with n interior grid points.\n",
    "\"\"\"\n",
    "\n",
    "function beaminit(n, dt, lambda = 20.0)\n",
    "    D2 = Lap1d(n)\n",
    "    dx = 1.0 / (n + 1)\n",
    "    x = collect(dx:dx:1.0-dx)\n",
    "    UN = zeros(size(x))\n",
    "    bdata = (D2 = D2, x = x, dx = dx, dt = dt, lambda = lambda, UN = UN)\n",
    "end\n",
    "```\n",
    "The bdata structure has more that just ```D2``` and $\\lambda$. I data for the solver and the construction of the Jacobian.\n",
    "\n",
    "Finally, the function __Lap1D__ computes ```D2``` as a tridiagonal matrix. This is part of the LinearAlgebra package that is part of Julia.Base.\n",
    "\n",
    "```Julia\n",
    "\"\"\"\n",
    "Lap1d(n)\n",
    "\n",
    "returns -d^2/dx^2 on [0,1] zero BC\n",
    "\"\"\"\n",
    "function Lap1d(n)\n",
    "    dx = 1 / (n + 1)\n",
    "    d = 2.0 * ones(n)\n",
    "    sup = -ones(n - 1)\n",
    "    slo = -ones(n - 1)\n",
    "    D2 = Tridiagonal(slo, d, sup)\n",
    "    D2 = D2 / (dx * dx)\n",
    "    return D2\n",
    "end\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.9 Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on the H-equation\n",
    "\n",
    "- Run the H-equation tests with $c=.99$ and larger dimensions ($2^k$ for $k=12, 13, 14$). Are the values of ```sham``` and ```resdec``` optimal for this case? How, for example, is ```resdec = Inf``` (your author's favorite value). To see why I like ```sham=Inf``` have a look at\n",
    "<cite data-cite=\"brent\"><a href=\"siamfa.html#brent\">(Bre73)</cite>.\n",
    "\n",
    "- What happens with $c=1$? Is the chord method a good idea? \n",
    "Don't look at\n",
    "<cite data-cite=\"ctk:sirev20\"><a href=\"siamfa.html#ctk:sirev20\">(Kel20a)</cite>,\n",
    "<cite data-cite=\"ctk:n1\"><a href=\"siamfa.html#ctk:n1\">(DK80)</cite>, or\n",
    "<cite data-cite=\"ctk:chord\"><a href=\"siamfa.html#ctk:chord\">(DK83)</cite> before trying to figure things out on your own.\n",
    "\n",
    "- Increase the dimension as much as you can and compare single and double precision linear algebra.\n",
    "    \n",
    "- Duplicate the table on page 125 of <cite data-cite=\"chand\"><a href=\"siamfa.html#chand\">(Cha60)</cite>.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Not trivial!)__ Try using __automatic differention__ to compute the Jacobian for the examples in this chapter and use the results to write a Jacobian evaluation function to pass to __nsol.jl__.\n",
    "You will first have to decide how you want to do that. Two of mature packages are [ForwardDiff.jl](#https://github.com/JuliaDiff/ForwardDiff.jl)\n",
    "<cite data-cite=\"forwarddiff\"><a href=\"siamfa.html#forwarddiff\">(RPL16)</cite>\n",
    "and [Zygote.jl](#https://github.com/FluxML/Zygote.jl) \n",
    "<cite data-cite=\"zygote\"><a href=\"siamfa.html#zygote\">(CoRR)</cite>.\n",
    "Books like\n",
    "<cite data-cite=\"grautodiff\"><a href=\"siamfa.html#grautodiff\">(Gri00)</cite> explain the\n",
    "algorithmic differences between these packages.\n",
    "    \n",
    "How does the performance compare to an analytic or forward difference Jacobian? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving a differential or integral equation\n",
    "by __nested iteration__ or __grid sequencing__ means resolving\n",
    "the rough features of the solution of a differential or\n",
    "integral equation on a coarse mesh, interpolating the\n",
    "solution to a finer mesh, resolving on the finer mesh, and\n",
    "then repeating the process until you have a solution on a target\n",
    "mesh.\n",
    "\n",
    "Apply this idea to some of the examples in the text, using piecewise\n",
    "linear interpolation to move from coarse to fine meshes. If the\n",
    "discretization is second-order accurate and you halve the mesh\n",
    "width at each level, how should you terminate the solver at\n",
    "each level? What kind of iteration statistics would tell you that\n",
    "you've done a satisfactory job?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Jacobian evaluation functions using sparse differencing for the boundary value problem and the buckling beam examples. You can use the literature, for example the methods from \n",
    "<cite data-cite=\"curtispr\"><a href=\"siamfa.html#curtisor\">(CPR74)</cite> or\n",
    "<cite data-cite=\"colmore\"><a href=\"siamfa.html#colmore\">(CM83)</cite>.  <cite data-cite=\"ctk:newton\"><a href=\"siamfa.html#ctk:newton\">(Kel03)</cite> has a Matlab code for banded matrices that, while convertible to Julia, needs some work to explictly store the Jacobian as a BandedMatrix so\n",
    "[BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl) will solve the linear system efficiently.\n",
    "Another alternative is to use __sparsefdiff__ \n",
    "<cite data-cite=\"sparsediff\"><a href=\"siamfa.html#sparsediff\">(RMGH20)</cite> from\n",
    "[SparseDiffTools.jl](#https://github.com/JuliaDiff/SparseDiffTools.jl). \n",
    "    \n",
    "No matter what you do, you'll need to think about fill-in, symbolic factorization in the general case, and storage. \n",
    "    \n",
    "Have fun!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ```lu!``` for Sparse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is for Julia 1.5 and higher. The option ```jfact=nofact``` in __nsol.jl__ is there so you can build and manage your own factorization inside your Jacobian evaluation function. Use this feature to store a factorization of the Jacobian for the PDE problem and reuse that storage rather than reallocating with every call to lu. What are the savings in time and allocations? You'll need to figure out how to use ```lu!``` for sparse matrices to do this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does $\\delta_0$ depend on $\\delta_x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary $\\delta_x$ and $\\delta_0$ in the buckling beam example. For \n",
    "the fixed $\\delta_x = 1/64$ in the example, explore the effects of\n",
    "increasing $\\delta_0$. Can you converge to the ``wrong'' (\\ie negative)\n",
    "solution in this way? What happens if you fix $\\delta_0$ and reduce\n",
    "$\\delta_x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next notebook = [Chapter 3: Newton-Krylov Methods](SIAMFANLCh3.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
