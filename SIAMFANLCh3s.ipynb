{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\md}{{\\bf D}}\n",
    "\\newcommand{\\mP}{{\\bf P}}\n",
    "\\newcommand{\\mU}{{\\bf U}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vw}{{\\bf w}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vf}{{\\bf f}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\vb}{{\\bf b}}\n",
    "\\newcommand{\\vz}{{\\bf z}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\mb}{{\\bf B}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "\\newcommand{\\begeq}{{\\begin{equation}}}\n",
    "\\newcommand{\\endeq}{{\\end{equation}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.7: Solvers for Chapter 3\n",
    "\n",
    "Contents for Section 3.7\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "[nsoli.jl](#nsoli.jl)\n",
    "\n",
    "- [Benchmarking the H-equation with nsoli.jl](#Benchmarking-the-H-equation-with-nsoli.jl)\n",
    "\n",
    "- [ Preconditioning the Convection-Diffusion Equation](#Preconditioning-the-Convection-Diffusion-Equation)\n",
    "\n",
    "[ptcsoli.jl](#ptcsoli.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the pattern of the previous chapters and present two solvers, a Newton code and a $\\ptc$ code. Both codes are for systems of equations and use Krylov methods to compute the step. We have two Krylov solvers, GMRES and BiCGstab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.7.1: nsoli.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nsoli.jl__ solves systems of nonlinear equations with Newton-Krylov methods. As usual, we begin with the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22m \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22mPDE \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mheq \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mPDE\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\n",
       "           fixedeta=true, Pvec=nothing, pside=\"right\",\n",
       "           armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "\\end{verbatim}\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsoli\n",
       "\n",
       "You must allocate storage for the function and the Krylov basis in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is an N x 1 column vector\n",
       "\n",
       "You may store it as (n,) or (n,1), depending on what F! likes to see.\n",
       "\n",
       "\n",
       "\\item FPS: preallocated storage for the Krylov basis. It is an N x m matrix where      you plan to take at most m-1 GMRES iterations before a restart. \n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item Jvec: Jacobian vector product, If you leave this out the   default is a finite difference directional derivative.\n",
       "\n",
       "So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "\n",
       "(v, FS, x) or (v, FS, x, pdata) must be the argument list,    even if FP does not need FS.   One reason for this is that the finite-difference derivative   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare FPS as Float64 or Float32 and nsoli    will do the right thing. Float16 support is there, but not working well.\n",
       "\n",
       "If the Jacobian is reasonably well conditioned, you can cut the cost   of orthogonalization and storage (for GMRES) in half with no loss.    There is no benefit if your linear solver is not GMRES or if    othogonalization and storage of the Krylov vectors is only a   small part of the cost of the computation. So if your preconditioner   is good and you only need a few Krylovs/Newton, reduced precision won't   help you much.\n",
       "\n",
       "BiCGSTAB does not benefit from reduced precsion. \n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m columns, and you need more than m-1 linear iterations, then GMRES  will restart. \n",
       "\n",
       "The default is -1 for GMRES. This means that you'll take m-1 iterations,  where size(V) = (n,m), and get no restarts. For BiCGSTAB the default is 10.\n",
       "\n",
       "lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only option for now.\n",
       "\n",
       "eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "where \n",
       "\n",
       "etag = eta if fixedeta=true\n",
       "\n",
       "etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "Pvec: Preconditioner-vector product. The rules are similar to Jvec     So, Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where     P(x) is the preconditioner. You must use x as an input even     if your preconditioner does not depend on x\n",
       "\n",
       "pside: apply preconditioner on pside, default = \"right\". I do not       recommend \"left\". See Chapter 3 for the story on this.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian-vector/Preconditioner-vector products.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of them.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "\\end{itemize}\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijac, iarm, ikfail), the  number of functions/Jacobian-vector prods/steplength reductions/linear solver failures at each iteration. Linear solver failures DO NOT mean that the nonlinear solver will fail. You should look at this stat if, for example, the line search fails. Increasing the size of FPS and/or lmaxit might solve the problem.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian-vector product.\n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\\begin{verbatim}\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "\\end{verbatim}\n",
       "– solhist:\n",
       "\n",
       "\\begin{verbatim}\n",
       "  This is the entire history of the iteration if you've set\n",
       "  keepsolhist=true\n",
       "\\end{verbatim}\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\subsubsection{Example from the docstrings for nsoli}\n",
       "\\paragraph{Simple 2D problem.}\n",
       "You should get the same results as for nsol.jl because GMRES will solve the equation for the step exactly in two iterations. Finite difference Jacobians and analytic Jacobian-vector products for full precision and finite difference Jacobian-vector products for single precision.\n",
       "\n",
       "BiCGSTAB converges in 5 itertions and each nonlinear iteration costs two Jacobian-vector products. Note that the storage for the Krylov space in GMRES (jvs) is replace by a single vector (fpv) when BiCGSTAB is the linear solver.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f! (generic function with 1 method)\n",
       "\n",
       "julia> function JVec(v, fv, x)\n",
       "       jvec=zeros(2,);\n",
       "       p=-sin(x[1]+x[2])\n",
       "       jvec[1]=v[1]+cos(x[2])*v[2]\n",
       "       jvec[2]=p*(v[1]+v[2])\n",
       "       return jvec\n",
       "       end\n",
       "JVec (generic function with 1 method)\n",
       "\n",
       "julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "\n",
       "julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\n",
       "\n",
       "julia> nout=nsol(f!,x0,fv,jv; sham=1);\n",
       "\n",
       "julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> [nout.history kout.history kout32.history]\n",
       "5×3 Array{Float64,2}:\n",
       " 1.88791e+00  1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01  2.43119e-01\n",
       " 1.19231e-02  1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03261e-05  1.03273e-05\n",
       " 1.46416e-11  1.40862e-11  1.45457e-11\n",
       "\n",
       "julia> fpv=zeros(2,);\n",
       "\n",
       "julia> koutb=nsoli(f!,x0,fv,fpv,JVec; fixedeta=true, eta=.1, lmaxit=2, \n",
       "       lsolver=\"bicgstab\");\n",
       "\n",
       "julia> koutb.history\n",
       "6-element Vector{Float64}:\n",
       " 1.88791e+00\n",
       " 2.43120e-01\n",
       " 1.19231e-02\n",
       " 4.87500e-04\n",
       " 7.54236e-06\n",
       " 3.84646e-07\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\n",
       "           fixedeta=true, Pvec=nothing, pside=\"right\",\n",
       "           armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "```\n",
       "\n",
       ")\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsoli\n",
       "\n",
       "You must allocate storage for the function and the Krylov basis in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is an N x 1 column vector\n",
       "\n",
       "    You may store it as (n,) or (n,1), depending on what F! likes to see.\n",
       "  * FPS: preallocated storage for the Krylov basis. It is an N x m matrix where      you plan to take at most m-1 GMRES iterations before a restart.\n",
       "\n",
       "  * Jvec: Jacobian vector product, If you leave this out the   default is a finite difference directional derivative.\n",
       "\n",
       "    So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "\n",
       "    (v, FS, x) or (v, FS, x, pdata) must be the argument list,    even if FP does not need FS.   One reason for this is that the finite-difference derivative   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare FPS as Float64 or Float32 and nsoli    will do the right thing. Float16 support is there, but not working well.\n",
       "\n",
       "    If the Jacobian is reasonably well conditioned, you can cut the cost   of orthogonalization and storage (for GMRES) in half with no loss.    There is no benefit if your linear solver is not GMRES or if    othogonalization and storage of the Krylov vectors is only a   small part of the cost of the computation. So if your preconditioner   is good and you only need a few Krylovs/Newton, reduced precision won't   help you much.\n",
       "\n",
       "    BiCGSTAB does not benefit from reduced precsion.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m columns, and you need more than m-1 linear iterations, then GMRES  will restart. \n",
       "\n",
       "The default is -1 for GMRES. This means that you'll take m-1 iterations,  where size(V) = (n,m), and get no restarts. For BiCGSTAB the default is 10.\n",
       "\n",
       "lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only option for now.\n",
       "\n",
       "eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "where \n",
       "\n",
       "etag = eta if fixedeta=true\n",
       "\n",
       "etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "Pvec: Preconditioner-vector product. The rules are similar to Jvec     So, Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where     P(x) is the preconditioner. You must use x as an input even     if your preconditioner does not depend on x\n",
       "\n",
       "pside: apply preconditioner on pside, default = \"right\". I do not       recommend \"left\". See Chapter 3 for the story on this.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian-vector/Preconditioner-vector products.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of them.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "  * A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijac, iarm, ikfail), the  number of functions/Jacobian-vector prods/steplength reductions/linear solver failures at each iteration. Linear solver failures DO NOT mean that the nonlinear solver will fail. You should look at this stat if, for example, the line search fails. Increasing the size of FPS and/or lmaxit might solve the problem.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian-vector product.\n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "```\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "```\n",
       "\n",
       "– solhist:\n",
       "\n",
       "```\n",
       "  This is the entire history of the iteration if you've set\n",
       "  keepsolhist=true\n",
       "```\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "---\n",
       "\n",
       "### Example from the docstrings for nsoli\n",
       "\n",
       "#### Simple 2D problem.\n",
       "\n",
       "You should get the same results as for nsol.jl because GMRES will solve the equation for the step exactly in two iterations. Finite difference Jacobians and analytic Jacobian-vector products for full precision and finite difference Jacobian-vector products for single precision.\n",
       "\n",
       "BiCGSTAB converges in 5 itertions and each nonlinear iteration costs two Jacobian-vector products. Note that the storage for the Krylov space in GMRES (jvs) is replace by a single vector (fpv) when BiCGSTAB is the linear solver.\n",
       "\n",
       "```jldoctest\n",
       "julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "       end\n",
       "f! (generic function with 1 method)\n",
       "\n",
       "julia> function JVec(v, fv, x)\n",
       "       jvec=zeros(2,);\n",
       "       p=-sin(x[1]+x[2])\n",
       "       jvec[1]=v[1]+cos(x[2])*v[2]\n",
       "       jvec[2]=p*(v[1]+v[2])\n",
       "       return jvec\n",
       "       end\n",
       "JVec (generic function with 1 method)\n",
       "\n",
       "julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "\n",
       "julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\n",
       "\n",
       "julia> nout=nsol(f!,x0,fv,jv; sham=1);\n",
       "\n",
       "julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\n",
       "\n",
       "julia> [nout.history kout.history kout32.history]\n",
       "5×3 Array{Float64,2}:\n",
       " 1.88791e+00  1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01  2.43119e-01\n",
       " 1.19231e-02  1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03261e-05  1.03273e-05\n",
       " 1.46416e-11  1.40862e-11  1.45457e-11\n",
       "\n",
       "julia> fpv=zeros(2,);\n",
       "\n",
       "julia> koutb=nsoli(f!,x0,fv,fpv,JVec; fixedeta=true, eta=.1, lmaxit=2, \n",
       "       lsolver=\"bicgstab\");\n",
       "\n",
       "julia> koutb.history\n",
       "6-element Vector{Float64}:\n",
       " 1.88791e+00\n",
       " 2.43120e-01\n",
       " 1.19231e-02\n",
       " 4.87500e-04\n",
       " 7.54236e-06\n",
       " 3.84646e-07\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  nsoli(F!, x0, FS, FPS, Jvec=dirder; rtol=1.e-6, atol=1.e-12,\u001b[39m\n",
       "\u001b[36m             maxit=20, lmaxit=-1, lsolver=\"gmres\", eta=.1,\u001b[39m\n",
       "\u001b[36m             fixedeta=true, Pvec=nothing, pside=\"right\",\u001b[39m\n",
       "\u001b[36m             armmax=10, dx = 1.e-7, armfix=false, pdata = nothing,\u001b[39m\n",
       "\u001b[36m             printerr = true, keepsolhist = false, stagnationok=false)\u001b[39m\n",
       "\n",
       "  )\n",
       "\n",
       "  C. T. Kelley, 2021\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: nsoli\n",
       "\n",
       "  You must allocate storage for the function and the Krylov basis in advance\n",
       "  –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •  F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "       your preallocated storage for the function.\n",
       "       So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "    •  x0: initial iterate\n",
       "\n",
       "    •  FS: Preallocated storage for function. It is an N x 1 column\n",
       "       vector\n",
       "       You may store it as (n,) or (n,1), depending on what F! likes to\n",
       "       see.\n",
       "\n",
       "    •  FPS: preallocated storage for the Krylov basis. It is an N x m\n",
       "       matrix where you plan to take at most m-1 GMRES iterations before\n",
       "       a restart.\n",
       "\n",
       "    •  Jvec: Jacobian vector product, If you leave this out the default\n",
       "       is a finite difference directional derivative.\n",
       "       So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v.\n",
       "       (v, FS, x) or (v, FS, x, pdata) must be the argument list, even if\n",
       "       FP does not need FS. One reason for this is that the\n",
       "       finite-difference derivative does and that is the default in the\n",
       "       solver.\n",
       "\n",
       "    •  Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "       full precision functions and linear algebra in any precision you\n",
       "       want. You can declare FPS as Float64 or Float32 and nsoli will do\n",
       "       the right thing. Float16 support is there, but not working well.\n",
       "       If the Jacobian is reasonably well conditioned, you can cut the\n",
       "       cost of orthogonalization and storage (for GMRES) in half with no\n",
       "       loss. There is no benefit if your linear solver is not GMRES or if\n",
       "       othogonalization and storage of the Krylov vectors is only a small\n",
       "       part of the cost of the computation. So if your preconditioner is\n",
       "       good and you only need a few Krylovs/Newton, reduced precision\n",
       "       won't help you much.\n",
       "       BiCGSTAB does not benefit from reduced precsion.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  maxit: limit on nonlinear iterations\n",
       "\n",
       "  lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m\n",
       "  columns, and you need more than m-1 linear iterations, then GMRES will\n",
       "  restart.\n",
       "\n",
       "  The default is -1 for GMRES. This means that you'll take m-1 iterations,\n",
       "  where size(V) = (n,m), and get no restarts. For BiCGSTAB the default is 10.\n",
       "\n",
       "  lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "  Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only\n",
       "  option for now.\n",
       "\n",
       "  eta and fixed eta: eta > 0 or there's an error\n",
       "\n",
       "  The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "  where\n",
       "\n",
       "  etag = eta if fixedeta=true\n",
       "\n",
       "  etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "  The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "  Pvec: Preconditioner-vector product. The rules are similar to Jvec So,\n",
       "  Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where P(x) is the\n",
       "  preconditioner. You must use x as an input even if your preconditioner does\n",
       "  not depend on x\n",
       "\n",
       "  pside: apply preconditioner on pside, default = \"right\". I do not recommend\n",
       "  \"left\". See Chapter 3 for the story on this.\n",
       "\n",
       "  armmax: upper bound on step size reductions in line search\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "  armfix: default = false\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the step\n",
       "  size will be fixed at .5. Don't do this unless you are doing experiments for\n",
       "  research.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian-vector/Preconditioner-vector\n",
       "  products. Things will go better if you use this rather than hide the data in\n",
       "  global variables within the module for your function/Jacobian\n",
       "\n",
       "  If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of\n",
       "  them.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  stagnationok: default = false\n",
       "\n",
       "  Set this to true if you want to disable the line search and either observe\n",
       "  divergence or stagnation. This is only useful for research or writing a\n",
       "  book.\n",
       "\n",
       "  Output:\n",
       "\n",
       "    •  A named tuple (solution, functionval, history, stats, idid,\n",
       "       errcode, solhist)\n",
       "\n",
       "  where\n",
       "\n",
       "  – solution = converged result\n",
       "\n",
       "  – functionval = F(solution)\n",
       "\n",
       "  – history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "  – stats = named tuple of the history of (ifun, ijac, iarm, ikfail), the\n",
       "  number of functions/Jacobian-vector prods/steplength reductions/linear\n",
       "  solver failures at each iteration. Linear solver failures DO NOT mean that\n",
       "  the nonlinear solver will fail. You should look at this stat if, for\n",
       "  example, the line search fails. Increasing the size of FPS and/or lmaxit\n",
       "  might solve the problem.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian-vector product.\n",
       "\n",
       "  – idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  – errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\u001b[36m      = -1 if the initial iterate satisfies the termination criteria\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 10 if no convergence after maxit iterations\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 1  if the line search failed\u001b[39m\n",
       "\n",
       "  – solhist:\n",
       "\n",
       "\u001b[36m    This is the entire history of the iteration if you've set\u001b[39m\n",
       "\u001b[36m    keepsolhist=true\u001b[39m\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Example from the docstrings for nsoli\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[1m  Simple 2D problem.\u001b[22m\n",
       "\u001b[1m  --------------------\u001b[22m\n",
       "\n",
       "  You should get the same results as for nsol.jl because GMRES will solve the\n",
       "  equation for the step exactly in two iterations. Finite difference Jacobians\n",
       "  and analytic Jacobian-vector products for full precision and finite\n",
       "  difference Jacobian-vector products for single precision.\n",
       "\n",
       "  BiCGSTAB converges in 5 itertions and each nonlinear iteration costs two\n",
       "  Jacobian-vector products. Note that the storage for the Krylov space in\n",
       "  GMRES (jvs) is replace by a single vector (fpv) when BiCGSTAB is the linear\n",
       "  solver.\n",
       "\n",
       "\u001b[36m  julia> function f!(fv,x)\u001b[39m\n",
       "\u001b[36m         fv[1]=x[1] + sin(x[2])\u001b[39m\n",
       "\u001b[36m         fv[2]=cos(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  f! (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> function JVec(v, fv, x)\u001b[39m\n",
       "\u001b[36m         jvec=zeros(2,);\u001b[39m\n",
       "\u001b[36m         p=-sin(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m         jvec[1]=v[1]+cos(x[2])*v[2]\u001b[39m\n",
       "\u001b[36m         jvec[2]=p*(v[1]+v[2])\u001b[39m\n",
       "\u001b[36m         return jvec\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  JVec (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x0=ones(2,); fv=zeros(2,); jv=zeros(2,2); jv32=zeros(Float32,2,2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> jvs=zeros(2,3); jvs32=zeros(Float32,2,3);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> nout=nsol(f!,x0,fv,jv; sham=1);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> kout=nsoli(f!,x0,fv,jvs,JVec; fixedeta=true, eta=.1, lmaxit=2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> kout32=nsoli(f!,x0,fv,jvs32; fixedeta=true, eta=.1, lmaxit=2);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [nout.history kout.history kout32.history]\u001b[39m\n",
       "\u001b[36m  5×3 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00  1.88791e+00  1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43119e-01  2.43120e-01  2.43119e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02  1.19231e-02  1.19231e-02\u001b[39m\n",
       "\u001b[36m   1.03266e-05  1.03261e-05  1.03273e-05\u001b[39m\n",
       "\u001b[36m   1.46416e-11  1.40862e-11  1.45457e-11\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> fpv=zeros(2,);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> koutb=nsoli(f!,x0,fv,fpv,JVec; fixedeta=true, eta=.1, lmaxit=2, \u001b[39m\n",
       "\u001b[36m         lsolver=\"bicgstab\");\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> koutb.history\u001b[39m\n",
       "\u001b[36m  6-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43120e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02\u001b[39m\n",
       "\u001b[36m   4.87500e-04\u001b[39m\n",
       "\u001b[36m   7.54236e-06\u001b[39m\n",
       "\u001b[36m   3.84646e-07\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nsoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.7.2: Benchmarking the H-equation with nsoli.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by comparing the fastest solution from Chapter 2 with two variants of Newton-GMRES, one with fixed $\\eta = .1$ and one with the Eisenstat-Walker forcing term with $\\eta_{max}=.9$ and $\\gamma = .9$. I'll allocate 20 vectors for the Krylov basis in the array FPK.\n",
    "\n",
    "We'll begin with a small version of the problem and compare the iteration statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=512;\n",
    "FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "bargs=(atol = 1.e-10, rtol = 1.e-10, sham = 5, resdec = .1, pdata=hdata);\n",
    "FPK=zeros(n,20);\n",
    "# Fixed eta = .1\n",
    "kbargs=(atol = 1.e-10, rtol = 1.e-10, eta=.1, fixedeta=true, pdata=hdata);\n",
    "# Eisenstat-Walker\n",
    "kbargsew=(atol = 1.e-10, rtol = 1.e-10, eta=.9, fixedeta=false, pdata=hdata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the winner from Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nout=nsol(heqf!, x0, FS, FPS32, heqJ!; bargs...);\n",
    "kout=nsoli(heqf!, x0, FS, FPK; kbargs...);\n",
    "koutew=nsoli(heqf!, x0, FS, FPK; kbargsew...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to compare the residual histories. They are essentially the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×3 Matrix{Float64}:\n",
       " 3.49504e+00  3.49504e+00  3.49504e+00\n",
       " 1.79697e-02  4.98627e-02  4.98627e-02\n",
       " 1.55514e-04  1.84641e-03  1.84641e-03\n",
       " 1.33168e-06  1.82364e-04  1.82364e-04\n",
       " 1.13963e-08  2.34291e-06  2.34291e-06\n",
       " 9.75293e-11  2.42540e-11  2.42540e-11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nout.history kout.history koutew.history]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the costs is harder. While a Jacobian-vector product for this problem has the same cost as a call to the function, the cost per iteration for nsol.jl is harder to evaluate in these terms. It's better to look at the benchmark results for a larger problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4096;\n",
    "FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "bargs=(atol = 1.e-10, rtol = 1.e-10, sham = 5, resdec = .1, pdata=hdata);\n",
    "FPK=zeros(n,20);\n",
    "kbargs=(atol = 1.e-10, rtol = 1.e-10, eta=.1, fixedeta=true, pdata=hdata);\n",
    "kbargsew=(atol = 1.e-10, rtol = 1.e-10, eta=.9, fixedeta=false, pdata=hdata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shamanskii, n=5\n",
      "  427.763 ms (8271 allocations: 1.10 MiB)\n",
      "Newton-GMRES, fixed eta\n",
      "  2.388 ms (383 allocations: 1.35 MiB)\n",
      "Newton-GMRES, Eisenstat-Walker\n",
      "  2.386 ms (383 allocations: 1.35 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"Shamanskii, n=5\"); @btime nsol(heqf!, $x0, $FS, $FPS32, heqJ!; bargs...);\n",
    "println(\"Newton-GMRES, fixed eta\"); @btime nsoli(heqf!, $x0, $FS, $FPK; kbargs...);\n",
    "println(\"Newton-GMRES, Eisenstat-Walker\"); @btime nsoli(heqf!, $x0, $FS, $FPK; kbargsew...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Krylov code is over 50 times faster. This is not unique to this problem. If your Jacobian is well-conditioned or you have a good preconditioner, as we do in the PDE example, Newton-Krylov should perform much better than any variation of Newton's method using direct linear solvers.\n",
    "\n",
    "The other interesting thing in this example is that the two forcing term choices performed equally well. \n",
    "\n",
    "Finally we will see if storing the Krylov basis in single precision improves matters. It's easy to do this by simply replacing ```FPK``` with ```FPK32```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton-GMRES, fixed eta\n",
      "  2.444 ms (384 allocations: 1.34 MiB)\n",
      "Newton-GMRES, Eisenstat-Walker\n",
      "  2.452 ms (384 allocations: 1.34 MiB)\n"
     ]
    }
   ],
   "source": [
    "#n=4096;\n",
    "#FS=ones(n,); FPS=ones(n,n); FPS32=ones(Float32,n,n); x0=ones(n,); c=.5; hdata = heqinit(x0, c);\n",
    "FPK32=zeros(Float32,n,20)\n",
    "println(\"Newton-GMRES, fixed eta\"); @btime nsoli(heqf!, $x0, $FS, $FPK32; kbargs...);\n",
    "println(\"Newton-GMRES, Eisenstat-Walker\"); @btime nsoli(heqf!, $x0, $FS, $FPK32; kbargsew...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is essentially no difference between storing the basis in single and double. It is easy in hindsight to see why. Each function evaluation and forward difference Jacobian-vector product is $O(N \\log N)$ work. The cost of othogonalization for $k$ GMRES iterations with classical Gram-Schmidt twice is $k^2 N$ (can you see why). So if we do $k$ Krylov iterations per Newton the cost of orthogonalization is $k^2 N$ and the cost of calls to the residual is $O(k N \\log N)$. The computation is dominated by the calls to the residual unless $k$ is very large. \n",
    "\n",
    "We will quantify this with a computation to look at the iteration statistics. It is sufficient to look at the\n",
    "fixed $\\eta = .1$ case. The results for the Eisenstat-Walker forcing term are exactly the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "fixedetaout = nsoli(heqf!, x0, FS, FPK; kbargs...);\n",
    "println(fixedetaout.stats.ijac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics indicate that we converge after a single GMRES iteration and are taking a single Krylov per Newton for most of the iteration (remember that the initial iteration is $\\vs = 0$ when computing the Newton step). So the orthogonalization cost is $N$ and the function evaluation cost is $O(N \\log N)$. We would expect that storing the Krylov basis  in single precision would have very little benefit, and that is exactly what we see.\n",
    "\n",
    "We invite the reader to increase $c$ and the dimension of the problem to see if anything changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.7.3: Preconditioning the Convection-Diffusion Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will benchmark the Newton-GMRES iteration agains the direct solvers from Chapter 2 and explore the differences between left and right preconditioning. We will begin by repeating the computation for the fastest version using __nsol.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsol, sham=5\n",
      "  9.751 ms (387 allocations: 6.55 MiB)\n"
     ]
    }
   ],
   "source": [
    "n=31;\n",
    "# Get some room for the residual\n",
    "u0=zeros(n*n,);\n",
    "FV=copy(u0);\n",
    "# Get the precomputed data from pdeinit\n",
    "pdata=pdeinit(n)\n",
    "# Storage for the Jacobian, same sparsity pattern as the discrete Laplacian\n",
    "J=copy(pdata.D2);\n",
    "# Iteration Parameters\n",
    "rtol=1.e-7\n",
    "atol=1.e-10\n",
    "println(\"nsol, sham=5\"); @btime nsol(pdeF!, $u0, $FV, $J, pdeJ!; resdec=.5, rtol=rtol, atol=atol, pdata=pdata, sham=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll set up the problem for nsoli. We need to allocate storage for the Krylov basis. One case will be no preconditioning at all, so the Kryov basis will need more storage. The analytic Jacobian-vector product is __Jvec2d.jl__, which is in __TestProblems/EllipticPDE.jl__. The preconditioner is __Pvec2d.jl__ from __TestProblems/PDE_Tools.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, not preconditioned\n",
      "  4.757 ms (3946 allocations: 1.06 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Storage for the Krylov basis\n",
    "    JV = zeros(n * n, 100)\n",
    "    eta=.1\n",
    "    fixedeta=false\n",
    "println(\"nsoli, not preconditioned\")\n",
    "@btime nsoli(pdeF!, $u0, $FV, $JV, Jvec2d; rtol=rtol, atol=atol, Pvec=nothing, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with no preconditioning, the iterative solver is almost as fast as __nsol.jl__ using the direct method. When you precondition, which we will do from the right for now, the difference is a factor of almost two over the solve without preconditioning. This difference would increase with a finer mesh. Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, Eisenstat-Walker forcing term\n",
      "  2.641 ms (970 allocations: 700.83 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"nsoli, preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, $u0, $FV, $JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will benchmark with a fixed forcing term for our next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, fixed eta\n",
      "  3.344 ms (1245 allocations: 1002.52 KiB)\n"
     ]
    }
   ],
   "source": [
    "fixedeta=true;\n",
    "println(\"nsoli, preconditioned, fixed eta\")\n",
    "@btime nsoli(pdeF!, $u0, $FV, $JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we see that Eisenstat-Walker is a bit better. Finally, we return to Eisenstat-Walker with $\\eta_{max} = .9$. We see very little difference from $\\eta_{max}=.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, preconditioned, Eisenstat-Walker forcing term\n",
      "  2.569 ms (1001 allocations: 797.09 KiB)\n"
     ]
    }
   ],
   "source": [
    "eta=.9; fixedeta=false;\n",
    "println(\"nsoli, preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, $u0, FV, $JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left preconditioning? We'll see that even with $\\eta_{max}=.1$ it's a bit slower that right preconditioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsoli, left preconditioned, Eisenstat-Walker forcing term\n",
      "  2.797 ms (1112 allocations: 803.78 KiB)\n"
     ]
    }
   ],
   "source": [
    "eta=.1\n",
    "fixedeta=false\n",
    "println(\"nsoli, left preconditioned, Eisenstat-Walker forcing term\")\n",
    "@btime nsoli(pdeF!, $u0, $FV, $JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta,\n",
    "            fixedeta=fixedeta, pside=\"left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try left preconditioning with $\\eta_{max} = .9$. We plotted the results in Figure 3.3. While the number of nonlinear iterations is roughly double that of the right preconditioned version, the solver time is less than the number of nonlinear iterations would indicate. Can you figure out why that is?\n",
    "\n",
    "Note that we have to increase ```maxit``` to give the nonlinear solver enough iterations to overcome the poor choice of preconditioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.736 ms (2105 allocations: 2.24 MiB)\n"
     ]
    }
   ],
   "source": [
    "eta=.9;\n",
    "@btime nsoli(pdeF!, $u0, $FV, $JV, Jvec2d; rtol=rtol, atol=atol, Pvec=Pvec2d, pdata=pdata, eta=eta, maxit=100,\n",
    "            fixedeta=fixedeta, pside=\"left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ptcsoli.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ptcsoli.jl__ is our Newton-Krylov $\\ptc$ code. Herewith the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mi\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc \u001b[0m\u001b[1mP\u001b[22mar\u001b[0m\u001b[1mt\u001b[22mialQui\u001b[0m\u001b[1mc\u001b[22mk\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "function ptcsoli(     F!,     x0,     FS,     FPS,     Jvec = dirder;     rtol = 1.e-6,     atol = 1.e-12,     maxit = 20,     lmaxit = -1,     lsolver = \"gmres\",     eta = 0.1,     fixedeta = true,     Pvec = nothing,     PvecKnowsdelta = false,      pside = \"right\",     delta0 = 1.e-6,     dx = 1.e-7,     pdata = nothing,     printerr = true,     keepsolhist = false, )\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsoli\n",
       "\n",
       "PTC finds the steady-state solution of u' = -F(u), u(0) = u\\_0. The - sign is a convention.\n",
       "\n",
       "You must allocate storage for the function and Krylov basis in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is an N x 1 column vector.\n",
       "\n",
       "\\end{itemize}\n",
       "You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can deal with it either way.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item FPS: preallocated storage for the Krylov basis. It is an N x m matrix where      you plan to take at most m-1 GMRES iterations before a restart. \n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item Jvec: Jacobian vector product, If you leave this out the   default is a finite difference directional derivative.\n",
       "\n",
       "So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "\n",
       "(v, FS, x) or (v, FS, x, pdata) must be the argument list,   even if FP does not need FS.   One reason for this is that the finite-difference derivative   does and that is the default in the solver.\n",
       "\n",
       "\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare FPS as Float64 or Float32 and ptcsoli    will do the right thing. Float16 support is there, but not working well.\n",
       "\n",
       "If the Jacobian is reasonably well conditioned, you can cut the cost   of orthogonalization and storage (for GMRES) in half with no loss.   There is no benefit if your linear solver is not GMRES or if   othogonalization and storage of the Krylov vectors is only a   small part of the cost of the computation. So if your preconditioner   is good and you only need a few Krylovs/Newton, reduced precision won't   help you much.\n",
       "\n",
       "BiCGSTAB does not benefit from reduced precsion.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "delta0: initial pseudo time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to delta0. If your choice of delta0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a straight-up nonlinear solve. This is part of the price for finding the  stable solution. \n",
       "\n",
       "lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m columns, and you need more than m-1 linear iterations, then GMRES will restart.\n",
       "\n",
       "The default is -1. For GMRES this means that you'll take m-1 iterations, where size(V) = (n,m), and get no restarts. For BiCGSTAB you'll then get the default of 10 iterations.\n",
       "\n",
       "lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only option for now. \n",
       "\n",
       "eta and fixed eta: eta > 0 or there's an error.\n",
       "\n",
       "The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "where\n",
       "\n",
       "etag = eta if fixedeta=true\n",
       "\n",
       "etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "The default, which may change, is eta=.1, fixedeta=true \n",
       "\n",
       "Pvec: Preconditioner-vector product. The rules are similar to Jvec     So, Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where     P(x) is the preconditioner. You must use x as an input even     if your preconditioner does not depend on x.\n",
       "\n",
       "PvecKnowsdelta: If you want your preconditioner-vector product to depend on      the pseudo-timestep delta, put an array deltaval in your precomputed     data. Initialize it as     deltaval = zeros(1,)     and let ptcsoli know about it by setting the kwarg     PvecKnowsdelta = true     ptcsoli will update the value in deltaval with every change     to delta with pdata.deltaval[1]=delta     so your preconditioner-vector product can get to it.\n",
       "\n",
       "pside: apply preconditioner on pside, default = \"right\". I do not       recommend \"left\". The problem with \"left\" for ptcsoli is       that it can fail to satisfy the inexact Newton condition for        the unpreconditioned equation, especially early in the iteration       and lead to an incorrect result (unstable solution or wrong        branch of steady state).       See Chapter 3 for the story on this. \n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-8 \n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian-vector/Preconditioner-vector products.  Things will go better if you use this rather than hide the data in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of them. precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian. \n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false. \n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, ikfail), the number of functions/jacobian-vector prodcuts/linear solver filures at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian-vector product.\n",
       "\n",
       "Linear solver failures need not cause the nonlinear iteration to fail.  You get a warning and that is all. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not. \n",
       "\n",
       "errcode = 0 if if the iteration succeeded \n",
       "\n",
       "\\begin{verbatim}\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\\end{verbatim}\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\subsubsection{Example from the docstrings for ptcsol}\n",
       "\\paragraph{The buckling beam problem.}\n",
       "You'll need to use TestProblems for this to work. The preconditioner is a solver for the high order term.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> using SIAMFANLEquations.TestProblems\n",
       "\n",
       "julia> function PreCondBeam(v, x, bdata)\n",
       "          J = bdata.D2\n",
       "          ptv = J\u000b",
       "\n",
       "       end\n",
       "PreCondBeam (generic function with 1 method)\n",
       "\n",
       "julia> n=63; maxit=1000; delta0 = 0.01; lambda = 20.0;\n",
       "\n",
       "julia> bdata = beaminit(n, 0.0, lambda);\n",
       "\n",
       "julia> x = bdata.x; u0 = x .* (1.0 .- x) .* (2.0 .- x); u0 .*= exp.(-10.0 * u0);\n",
       "\n",
       "\n",
       "julia> FS = copy(u0); FPJV=zeros(n,20);\n",
       "\n",
       "julia> pout = ptcsoli( FBeam!, u0, FS, FPJV; delta0 = delta0, pdata = bdata,\n",
       "       eta = 1.e-2, rtol = 1.e-10, maxit = maxit, Pvec = PreCondBeam);\n",
       "\n",
       "julia> # It takes a few iterations to get there.\n",
       "       length(pout.history)\n",
       "25\n",
       "\n",
       "julia> [pout.history[1:5] pout.history[21:25]]\n",
       "5×2 Array{Float64,2}:\n",
       " 6.31230e+01  1.79578e+00\n",
       " 7.45926e+00  2.65964e-01\n",
       " 8.73598e+00  6.58278e-03\n",
       " 2.91936e+01  8.35069e-06\n",
       " 3.47969e+01  5.11594e-09\n",
       "\n",
       "julia> # We get the nonnegative stedy state.\n",
       "       norm(pout.solution,Inf)\n",
       "2.19086e+00\n",
       "\n",
       "n=63; maxit=1000; delta0 = 0.01; lambda = 20.0;\n",
       "\n",
       "julia> # Use BiCGSTAB for the linear solver\n",
       "\n",
       "julia> FS = copy(u0); FPJV=zeros(n,);\n",
       "\n",
       "julia> pout = ptcsoli( FBeam!, u0, FS, FPJV; delta0 = delta0, pdata = bdata,\n",
       "       eta = 1.e-2, rtol = 1.e-10, maxit = maxit, \n",
       "       Pvec = PreCondBeam, lsolver=\"bicgstab\");\n",
       "\n",
       "julia> # Same number of iterations as GMRES, but each one costs double \n",
       "\n",
       "julia> # the Jacobian-vector products and much less storage\n",
       "\n",
       "julia> length(pout.history)\n",
       "25\n",
       "\n",
       "julia> [pout.history[1:5] pout.history[21:25]]\n",
       "5×2 Matrix{Float64}:\n",
       " 6.31230e+01  1.68032e+00\n",
       " 7.47081e+00  2.35073e-01\n",
       " 8.62095e+00  5.18262e-03\n",
       " 2.96495e+01  3.23715e-06\n",
       " 3.51504e+01  3.33107e-10\n",
       "\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "function ptcsoli(     F!,     x0,     FS,     FPS,     Jvec = dirder;     rtol = 1.e-6,     atol = 1.e-12,     maxit = 20,     lmaxit = -1,     lsolver = \"gmres\",     eta = 0.1,     fixedeta = true,     Pvec = nothing,     PvecKnowsdelta = false,      pside = \"right\",     delta0 = 1.e-6,     dx = 1.e-7,     pdata = nothing,     printerr = true,     keepsolhist = false, )\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: some new stuff ==> ptcsoli\n",
       "\n",
       "PTC finds the steady-state solution of u' = -F(u), u(0) = u_0. The - sign is a convention.\n",
       "\n",
       "You must allocate storage for the function and Krylov basis in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is an N x 1 column vector.\n",
       "\n",
       "You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can deal with it either way.\n",
       "\n",
       "  * FPS: preallocated storage for the Krylov basis. It is an N x m matrix where      you plan to take at most m-1 GMRES iterations before a restart.\n",
       "\n",
       "  * Jvec: Jacobian vector product, If you leave this out the   default is a finite difference directional derivative.\n",
       "\n",
       "    So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v. \n",
       "\n",
       "    (v, FS, x) or (v, FS, x, pdata) must be the argument list,   even if FP does not need FS.   One reason for this is that the finite-difference derivative   does and that is the default in the solver.\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare FPS as Float64 or Float32 and ptcsoli    will do the right thing. Float16 support is there, but not working well.\n",
       "\n",
       "    If the Jacobian is reasonably well conditioned, you can cut the cost   of orthogonalization and storage (for GMRES) in half with no loss.   There is no benefit if your linear solver is not GMRES or if   othogonalization and storage of the Krylov vectors is only a   small part of the cost of the computation. So if your preconditioner   is good and you only need a few Krylovs/Newton, reduced precision won't   help you much.\n",
       "\n",
       "    BiCGSTAB does not benefit from reduced precsion.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "delta0: initial pseudo time step. The default value of 1.e-3 is a bit conservative and is one option you really should play with. Look at the example where I set it to 1.0!\n",
       "\n",
       "maxit: limit on nonlinear iterations, default=100. \n",
       "\n",
       "This is coupled to delta0. If your choice of delta0 is too small (conservative) then you'll need many iterations to converge and will need a larger value of maxit\n",
       "\n",
       "For PTC you'll need more iterations than for a straight-up nonlinear solve. This is part of the price for finding the  stable solution. \n",
       "\n",
       "lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m columns, and you need more than m-1 linear iterations, then GMRES will restart.\n",
       "\n",
       "The default is -1. For GMRES this means that you'll take m-1 iterations, where size(V) = (n,m), and get no restarts. For BiCGSTAB you'll then get the default of 10 iterations.\n",
       "\n",
       "lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only option for now. \n",
       "\n",
       "eta and fixed eta: eta > 0 or there's an error.\n",
       "\n",
       "The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "where\n",
       "\n",
       "etag = eta if fixedeta=true\n",
       "\n",
       "etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "The default, which may change, is eta=.1, fixedeta=true \n",
       "\n",
       "Pvec: Preconditioner-vector product. The rules are similar to Jvec     So, Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where     P(x) is the preconditioner. You must use x as an input even     if your preconditioner does not depend on x.\n",
       "\n",
       "PvecKnowsdelta: If you want your preconditioner-vector product to depend on      the pseudo-timestep delta, put an array deltaval in your precomputed     data. Initialize it as     deltaval = zeros(1,)     and let ptcsoli know about it by setting the kwarg     PvecKnowsdelta = true     ptcsoli will update the value in deltaval with every change     to delta with pdata.deltaval[1]=delta     so your preconditioner-vector product can get to it.\n",
       "\n",
       "pside: apply preconditioner on pside, default = \"right\". I do not       recommend \"left\". The problem with \"left\" for ptcsoli is       that it can fail to satisfy the inexact Newton condition for        the unpreconditioned equation, especially early in the iteration       and lead to an incorrect result (unstable solution or wrong        branch of steady state).       See Chapter 3 for the story on this. \n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x)+1.e-8 \n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian-vector/Preconditioner-vector products.  Things will go better if you use this rather than hide the data in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of them. precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian. \n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false. \n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "A named tuple (solution, functionval, history, stats, idid,                errcode, solhist) where\n",
       "\n",
       "solution = converged result functionval = F(solution) history = the vector of residual norms (||F(x)||) for the iteration stats = named tuple of the history of (ifun, ijac, ikfail), the number of functions/jacobian-vector prodcuts/linear solver filures at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian-vector product.\n",
       "\n",
       "Linear solver failures need not cause the nonlinear iteration to fail.  You get a warning and that is all. \n",
       "\n",
       "idid=true if the iteration succeeded and false if not. \n",
       "\n",
       "errcode = 0 if if the iteration succeeded \n",
       "\n",
       "```\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "    = 10 if no convergence after maxit iterations\n",
       "```\n",
       "\n",
       "solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "### Example from the docstrings for ptcsol\n",
       "\n",
       "#### The buckling beam problem.\n",
       "\n",
       "You'll need to use TestProblems for this to work. The preconditioner is a solver for the high order term.\n",
       "\n",
       "```jldoctest\n",
       "julia> using SIAMFANLEquations.TestProblems\n",
       "\n",
       "julia> function PreCondBeam(v, x, bdata)\n",
       "          J = bdata.D2\n",
       "          ptv = J\u000b",
       "\n",
       "       end\n",
       "PreCondBeam (generic function with 1 method)\n",
       "\n",
       "julia> n=63; maxit=1000; delta0 = 0.01; lambda = 20.0;\n",
       "\n",
       "julia> bdata = beaminit(n, 0.0, lambda);\n",
       "\n",
       "julia> x = bdata.x; u0 = x .* (1.0 .- x) .* (2.0 .- x); u0 .*= exp.(-10.0 * u0);\n",
       "\n",
       "\n",
       "julia> FS = copy(u0); FPJV=zeros(n,20);\n",
       "\n",
       "julia> pout = ptcsoli( FBeam!, u0, FS, FPJV; delta0 = delta0, pdata = bdata,\n",
       "       eta = 1.e-2, rtol = 1.e-10, maxit = maxit, Pvec = PreCondBeam);\n",
       "\n",
       "julia> # It takes a few iterations to get there.\n",
       "       length(pout.history)\n",
       "25\n",
       "\n",
       "julia> [pout.history[1:5] pout.history[21:25]]\n",
       "5×2 Array{Float64,2}:\n",
       " 6.31230e+01  1.79578e+00\n",
       " 7.45926e+00  2.65964e-01\n",
       " 8.73598e+00  6.58278e-03\n",
       " 2.91936e+01  8.35069e-06\n",
       " 3.47969e+01  5.11594e-09\n",
       "\n",
       "julia> # We get the nonnegative stedy state.\n",
       "       norm(pout.solution,Inf)\n",
       "2.19086e+00\n",
       "\n",
       "n=63; maxit=1000; delta0 = 0.01; lambda = 20.0;\n",
       "\n",
       "julia> # Use BiCGSTAB for the linear solver\n",
       "\n",
       "julia> FS = copy(u0); FPJV=zeros(n,);\n",
       "\n",
       "julia> pout = ptcsoli( FBeam!, u0, FS, FPJV; delta0 = delta0, pdata = bdata,\n",
       "       eta = 1.e-2, rtol = 1.e-10, maxit = maxit, \n",
       "       Pvec = PreCondBeam, lsolver=\"bicgstab\");\n",
       "\n",
       "julia> # Same number of iterations as GMRES, but each one costs double \n",
       "\n",
       "julia> # the Jacobian-vector products and much less storage\n",
       "\n",
       "julia> length(pout.history)\n",
       "25\n",
       "\n",
       "julia> [pout.history[1:5] pout.history[21:25]]\n",
       "5×2 Matrix{Float64}:\n",
       " 6.31230e+01  1.68032e+00\n",
       " 7.47081e+00  2.35073e-01\n",
       " 8.62095e+00  5.18262e-03\n",
       " 2.96495e+01  3.23715e-06\n",
       " 3.51504e+01  3.33107e-10\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "  function ptcsoli( F!, x0, FS, FPS, Jvec = dirder; rtol = 1.e-6, atol =\n",
       "  1.e-12, maxit = 20, lmaxit = -1, lsolver = \"gmres\", eta = 0.1, fixedeta =\n",
       "  true, Pvec = nothing, PvecKnowsdelta = false, pside = \"right\", delta0 =\n",
       "  1.e-6, dx = 1.e-7, pdata = nothing, printerr = true, keepsolhist = false, )\n",
       "\n",
       "  C. T. Kelley, 2021\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: some\n",
       "  new stuff ==> ptcsoli\n",
       "\n",
       "  PTC finds the steady-state solution of u' = -F(u), u(0) = u_0. The - sign is\n",
       "  a convention.\n",
       "\n",
       "  You must allocate storage for the function and Krylov basis in advance –> in\n",
       "  the calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •  F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "       your preallocated storage for the function.\n",
       "       So, FV=F!(FV,x) or FV=F!(FV,x,pdata) returns FV=F(x)\n",
       "\n",
       "    •  x0: initial iterate\n",
       "\n",
       "    •  FS: Preallocated storage for function. It is an N x 1 column\n",
       "       vector.\n",
       "\n",
       "  You may dimension it as (n,) or (n,1). (n,) is best, but the solvers can\n",
       "  deal with it either way.\n",
       "\n",
       "    •  FPS: preallocated storage for the Krylov basis. It is an N x m\n",
       "       matrix where you plan to take at most m-1 GMRES iterations before\n",
       "       a restart.\n",
       "\n",
       "    •  Jvec: Jacobian vector product, If you leave this out the default\n",
       "       is a finite difference directional derivative.\n",
       "       So, FP=Jvec(v,FS,x) or FP=Jvec(v,FS,x,pdata) returns FP=F'(x) v.\n",
       "       (v, FS, x) or (v, FS, x, pdata) must be the argument list, even if\n",
       "       FP does not need FS. One reason for this is that the\n",
       "       finite-difference derivative does and that is the default in the\n",
       "       solver.\n",
       "\n",
       "    •  Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "       full precision functions and linear algebra in any precision you\n",
       "       want. You can declare FPS as Float64 or Float32 and ptcsoli will\n",
       "       do the right thing. Float16 support is there, but not working\n",
       "       well.\n",
       "       If the Jacobian is reasonably well conditioned, you can cut the\n",
       "       cost of orthogonalization and storage (for GMRES) in half with no\n",
       "       loss. There is no benefit if your linear solver is not GMRES or if\n",
       "       othogonalization and storage of the Krylov vectors is only a small\n",
       "       part of the cost of the computation. So if your preconditioner is\n",
       "       good and you only need a few Krylovs/Newton, reduced precision\n",
       "       won't help you much.\n",
       "       BiCGSTAB does not benefit from reduced precsion.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  delta0: initial pseudo time step. The default value of 1.e-3 is a bit\n",
       "  conservative and is one option you really should play with. Look at the\n",
       "  example where I set it to 1.0!\n",
       "\n",
       "  maxit: limit on nonlinear iterations, default=100.\n",
       "\n",
       "  This is coupled to delta0. If your choice of delta0 is too small\n",
       "  (conservative) then you'll need many iterations to converge and will need a\n",
       "  larger value of maxit\n",
       "\n",
       "  For PTC you'll need more iterations than for a straight-up nonlinear solve.\n",
       "  This is part of the price for finding the stable solution.\n",
       "\n",
       "  lmaxit: limit on linear iterations. If lmaxit > m-1, where FPS has m\n",
       "  columns, and you need more than m-1 linear iterations, then GMRES will\n",
       "  restart.\n",
       "\n",
       "  The default is -1. For GMRES this means that you'll take m-1 iterations,\n",
       "  where size(V) = (n,m), and get no restarts. For BiCGSTAB you'll then get the\n",
       "  default of 10 iterations.\n",
       "\n",
       "  lsolver: the linear solver, default = \"gmres\"\n",
       "\n",
       "  Your choices will be \"gmres\" or \"bicgstab\". However, gmres is the only\n",
       "  option for now.\n",
       "\n",
       "  eta and fixed eta: eta > 0 or there's an error.\n",
       "\n",
       "  The linear solver terminates when ||F'(x)s + F(x) || <= etag || F(x) ||\n",
       "\n",
       "  where\n",
       "\n",
       "  etag = eta if fixedeta=true\n",
       "\n",
       "  etag = Eisenstat-Walker as implemented in book if fixedeta=false\n",
       "\n",
       "  The default, which may change, is eta=.1, fixedeta=true\n",
       "\n",
       "  Pvec: Preconditioner-vector product. The rules are similar to Jvec So,\n",
       "  Pv=Pvec(v,x) or Pv=Pvec(v,x,pdata) returns P(x) v where P(x) is the\n",
       "  preconditioner. You must use x as an input even if your preconditioner does\n",
       "  not depend on x.\n",
       "\n",
       "  PvecKnowsdelta: If you want your preconditioner-vector product to depend on\n",
       "  the pseudo-timestep delta, put an array deltaval in your precomputed data.\n",
       "  Initialize it as deltaval = zeros(1,) and let ptcsoli know about it by\n",
       "  setting the kwarg PvecKnowsdelta = true ptcsoli will update the value in\n",
       "  deltaval with every change to delta with pdata.deltaval[1]=delta so your\n",
       "  preconditioner-vector product can get to it.\n",
       "\n",
       "  pside: apply preconditioner on pside, default = \"right\". I do not recommend\n",
       "  \"left\". The problem with \"left\" for ptcsoli is that it can fail to satisfy\n",
       "  the inexact Newton condition for the unpreconditioned equation, especially\n",
       "  early in the iteration and lead to an incorrect result (unstable solution or\n",
       "  wrong branch of steady state). See Chapter 3 for the story on this.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x)+1.e-8\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian-vector/Preconditioner-vector\n",
       "  products. Things will go better if you use this rather than hide the data in\n",
       "  global variables within the module for your function/Jacobian\n",
       "\n",
       "  If you use pdata in any of F!, Jvec, or Pvec, you must use in in all of\n",
       "  them. precomputed data for the function/Jacobian. Things will go better if\n",
       "  you use this rather than hide the data in global variables within the module\n",
       "  for your function/Jacobian.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  Output:\n",
       "\n",
       "  A named tuple (solution, functionval, history, stats, idid, errcode,\n",
       "  solhist) where\n",
       "\n",
       "  solution = converged result functionval = F(solution) history = the vector\n",
       "  of residual norms (||F(x)||) for the iteration stats = named tuple of the\n",
       "  history of (ifun, ijac, ikfail), the number of functions/jacobian-vector\n",
       "  prodcuts/linear solver filures at each iteration.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian-vector product.\n",
       "\n",
       "  Linear solver failures need not cause the nonlinear iteration to fail. You\n",
       "  get a warning and that is all.\n",
       "\n",
       "  idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\u001b[36m      = -1 if the initial iterate satisfies the termination criteria\u001b[39m\n",
       "\u001b[36m      = 10 if no convergence after maxit iterations\u001b[39m\n",
       "\n",
       "  solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iteration + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\u001b[1m  Example from the docstrings for ptcsol\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[1m  The buckling beam problem.\u001b[22m\n",
       "\u001b[1m  ----------------------------\u001b[22m\n",
       "\n",
       "  You'll need to use TestProblems for this to work. The preconditioner is a\n",
       "  solver for the high order term.\n",
       "\n",
       "\u001b[36m  julia> using SIAMFANLEquations.TestProblems\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> function PreCondBeam(v, x, bdata)\u001b[39m\n",
       "\u001b[36m            J = bdata.D2\u001b[39m\n",
       "\u001b[36m            ptv = J\u000b",
       "\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  PreCondBeam (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> n=63; maxit=1000; delta0 = 0.01; lambda = 20.0;\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> bdata = beaminit(n, 0.0, lambda);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x = bdata.x; u0 = x .* (1.0 .- x) .* (2.0 .- x); u0 .*= exp.(-10.0 * u0);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> FS = copy(u0); FPJV=zeros(n,20);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> pout = ptcsoli( FBeam!, u0, FS, FPJV; delta0 = delta0, pdata = bdata,\u001b[39m\n",
       "\u001b[36m         eta = 1.e-2, rtol = 1.e-10, maxit = maxit, Pvec = PreCondBeam);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # It takes a few iterations to get there.\u001b[39m\n",
       "\u001b[36m         length(pout.history)\u001b[39m\n",
       "\u001b[36m  25\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [pout.history[1:5] pout.history[21:25]]\u001b[39m\n",
       "\u001b[36m  5×2 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   6.31230e+01  1.79578e+00\u001b[39m\n",
       "\u001b[36m   7.45926e+00  2.65964e-01\u001b[39m\n",
       "\u001b[36m   8.73598e+00  6.58278e-03\u001b[39m\n",
       "\u001b[36m   2.91936e+01  8.35069e-06\u001b[39m\n",
       "\u001b[36m   3.47969e+01  5.11594e-09\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # We get the nonnegative stedy state.\u001b[39m\n",
       "\u001b[36m         norm(pout.solution,Inf)\u001b[39m\n",
       "\u001b[36m  2.19086e+00\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  n=63; maxit=1000; delta0 = 0.01; lambda = 20.0;\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # Use BiCGSTAB for the linear solver\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> FS = copy(u0); FPJV=zeros(n,);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> pout = ptcsoli( FBeam!, u0, FS, FPJV; delta0 = delta0, pdata = bdata,\u001b[39m\n",
       "\u001b[36m         eta = 1.e-2, rtol = 1.e-10, maxit = maxit, \u001b[39m\n",
       "\u001b[36m         Pvec = PreCondBeam, lsolver=\"bicgstab\");\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # Same number of iterations as GMRES, but each one costs double \u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> # the Jacobian-vector products and much less storage\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> length(pout.history)\u001b[39m\n",
       "\u001b[36m  25\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [pout.history[1:5] pout.history[21:25]]\u001b[39m\n",
       "\u001b[36m  5×2 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   6.31230e+01  1.68032e+00\u001b[39m\n",
       "\u001b[36m   7.47081e+00  2.35073e-01\u001b[39m\n",
       "\u001b[36m   8.62095e+00  5.18262e-03\u001b[39m\n",
       "\u001b[36m   2.96495e+01  3.23715e-06\u001b[39m\n",
       "\u001b[36m   3.51504e+01  3.33107e-10\u001b[39m\n",
       "\u001b[36m  \u001b[39m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ptcsoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking $\\ptc$ with the buckling beam problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set up the beam problem as we did before. Remember that ```bdata.D2``` is the discrete Laplacian in one space dimension, which we compute within the initialization function ```beaminit```. We will start with __ptcsol.jl__ to remind you what we did before and solve a larger problem to compare using a direct solver with GMRES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1023; lambda=20; delta=.01; maxit=1000; bdata = beaminit(n, 0.0, lambda); \n",
    "x = bdata.x; u0 = x .* (1.0 .- x) .* (2.0 .- x); u0 .*= exp.(-10.0 * u0);\n",
    "FS = copy(u0); FPS=copy(bdata.D2); FPJV = zeros(n, 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll benchmark the solve. Remember that ```FBeam!``` and ```BeamJ!``` are defined in the TestProblems submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.129 ms (695 allocations: 3.39 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime ptcsol(FBeam!, $u0, $FS, $FPS, BeamJ!; rtol=1.e-10, pdata=bdata, delta0=delta, maxit=maxit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test ptcsoli we will use the $\\delta$-dependent preconditioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ptvbeamdelta (generic function with 1 method)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ptvbeamdelta(v, x, bdata)\n",
    "    delta = bdata.deltaval[1]\n",
    "    J = bdata.D2 + (1.0 / delta) * I\n",
    "    ptv = J \\ v\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.142 ms (2821 allocations: 9.73 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime ptcsoli(FBeam!, $u0, $FS, FPJV; lsolver=\"gmres\", delta0=delta, pdata=bdata, lmaxit=19, eta=1.e-2,\n",
    "     Pvec=ptvbeamdelta, pside=\"right\", PvecKnowsdelta=true, maxit=maxit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the iterative linear solver costs nearly three times as much as the direct solver. This is no surprise as the application of the preconditioner requires a tridiagonal solve, which is the same cost as solving the equation for the Newton step with a direct method. The buckling beam problem is simply not hard enough to benefit from an iteraive linear solver. The reader should try increasing $n$ to see if anything changes, but should keep in mind that one may need to reduce $\\delta_0$ as $n$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.8:  Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Storage Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark the solves for the H-equation and the convection-diffusion equation using BiCGSTAB and GMRES(m) for the linear solvers. How do the runtimes and memory allocations compare to full GMRES? How do the runtimes and allocations depend on the dimension and $m$ for GMRES(m)? Do things change for $c=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesh Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An iteration for a discretization of a differential or integral equation is mesh-independent if the iteration statistics are independent of the grid. Nonlinear iterations are usually mesh-indepedent if the discretization is reasonably well-done <cite data-cite=\"allg\"><a href=\"siamfa.html#allg\">(ABPR86)</cite>. That is not the case, however, for the linear solves. One can only get mesh-independence for the linear solve if the preconditioner is so good that it essentially converts the problem into an integral equation. For the H-equation and the convection-diffusion (both preconditioned and not), vary the grid size and see how the iteration statistics change. Use both full GMRES and the low-storage solvers. You will want to make figures like the ones earlier in this chapter that plot residual norm against both the number of nonlinear iterations and the number of Jacobian-vector products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with the convection term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the convection term $C$ in the convection-diffusion equation\n",
    "$$\n",
    "-\\nabla^2 u + C u ( u_x + u_y) = f\n",
    "$$\n",
    "where you use the boundary conditions and exact solution $u^*$ from [Chapter 2](SIAMFANLCh2.ipynb). Hence the forcing term $f$ will depend on $c$. Vary $C$ from $C=20$ ( the choice in our examples ) to $C = 1000$ or larger. What happens to the linear and nonlinear iteration statistics? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next notebook = [Chapter 4: Fixed Point Problems  and Anderson Acceleration](SIAMFANLCh4.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
