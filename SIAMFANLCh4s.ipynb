{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\calk}{{\\cal K}}\n",
    "\\newcommand{\\calp}{{\\cal P}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\md}{{\\bf D}}\n",
    "\\newcommand{\\mP}{{\\bf P}}\n",
    "\\newcommand{\\mU}{{\\bf U}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\vx}{{\\bf x}}\n",
    "\\newcommand{\\vw}{{\\bf w}}\n",
    "\\newcommand{\\vy}{{\\bf y}}\n",
    "\\newcommand{\\vf}{{\\bf f}}\n",
    "\\newcommand{\\vs}{{\\bf s}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\vd}{{\\bf d}}\n",
    "\\newcommand{\\vb}{{\\bf b}}\n",
    "\\newcommand{\\vg}{{\\bf g}}\n",
    "\\newcommand{\\vz}{{\\bf z}}\n",
    "\\newcommand{\\vr}{{\\bf r}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mv}{{\\bf V}}\n",
    "\\newcommand{\\ma}{{\\bf A}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\mm}{{\\bf M}}\n",
    "\\newcommand{\\mb}{{\\bf B}}\n",
    "\\newcommand{\\ball}{{\\cal B}}\n",
    "\\newcommand{\\ptc}{{\\Psi TC}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "\\newcommand{\\begeq}{{\\begin{equation}}}\n",
    "\\newcommand{\\endeq}{{\\end{equation}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling SIAMFANLEquations [084e46ad-d928-497d-ad5e-07fa361a48c4]\n",
      "└ @ Base loading.jl:1342\n",
      "┌ Info: Precompiling BenchmarkTools [6e4b80f9-dd63-53aa-95a3-0cdb28fa8baf]\n",
      "└ @ Base loading.jl:1342\n",
      "┌ Info: Precompiling PyPlot [d330b81b-6aea-500a-939a-2ce795aea3ee]\n",
      "└ @ Base loading.jl:1342\n",
      "┌ Info: Precompiling NotebookSIAMFANL [top-level]\n",
      "└ @ Base loading.jl:1342\n"
     ]
    }
   ],
   "source": [
    "include(\"fanote_init.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.5:  Solver for Anderson Acceleration, aasol.jl\n",
    "\n",
    "Contents for Section 4.5\n",
    "\n",
    "[Overview](#Overview)\n",
    "\n",
    "[nsoli.jl](#aasol.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we begin with the docstrings. You should not expect the iteration stats to agree to high precision across operating systems because the optimization problems are so ill-conditioned. This is especially the case with the example from Toth/Kelley 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.5.1: aasol.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m p\u001b[0m\u001b[1ma\u001b[22mrti\u001b[0m\u001b[1ma\u001b[22ml\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrt p\u001b[0m\u001b[1ma\u001b[22mrti\u001b[0m\u001b[1ma\u001b[22ml\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrt! p\u001b[0m\u001b[1ma\u001b[22mrti\u001b[0m\u001b[1ma\u001b[22ml\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrtperm p\u001b[0m\u001b[1ma\u001b[22mrti\u001b[0m\u001b[1ma\u001b[22ml\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrtperm!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "aasol(GFix!, x0, m, Vstore; maxit=20,       rtol=1.e-10, atol=1.e-10, beta=1.0, pdata=nothing, keepsolhist = false)\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia code for Anderson acceleration. Nothing fancy.\n",
       "\n",
       "Solvers fixed point problems x = G(x).\n",
       "\n",
       "You must allocate storage for the function and fixed point map history –> in the calling program <– in the array Vstore.\n",
       "\n",
       "For an n dimensional problem with Anderson(m), Vstore must have at least 2m + 4 columns and 3m + 3 is better.  If m=0 (Picard) then V must have at least 4 columns.\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item GFix!: fixed-point map, the ! indicates that GFix! overwrites xout, your   preallocated storage for the function value xout=G(xin).\n",
       "\n",
       "So xout=GFix!(xout,xin) or xout=GFix!(xout,xin,pdata) returns   xout=G(xin)\n",
       "\n",
       "\n",
       "\\item x0: Initial iterate. It is a vector of size N\n",
       "\n",
       "You should store it as (N,) and design G! to use vectors of size (N,). If you use (N,1) consistently instead, the solvers may work, but I make no guarantees.\n",
       "\n",
       "\n",
       "\\item m: depth for Anderson acceleration. m=0 is Picard iteration\n",
       "\n",
       "\n",
       "\\item Vstore: Working storage array. For an n dimensional problem Vstore should have at least 3m+3 columns unless you are storage bound. If storage is a problem, then you can allocate a minimum of 2m+4 columns. The smaller allocation exacts a performance penalty, especially for small problems and small values of m. So for Anderson(3), Vstore should be no smaller  than zeros(N,8) with zeros(N,11) a better choice. Vstore needs to allocate for the history of differences of the residuals and fixed point maps. The extra m-1 columns are for storing intermediate results in the downdating phase of the QR factorization for the coefficeint  matrix of the optimization problem. See the notebook or the print book  for the details of this mess. \n",
       "\n",
       "If m=0, then Vstore needs 4 columns.\n",
       "\n",
       "\\end{itemize}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "maxit: default = 20\n",
       "\n",
       "limit on nonlinear iterations\n",
       "\n",
       "rtol and atol: default = 1.e-10\n",
       "\n",
       "relative and absolute error tolerances\n",
       "\n",
       "beta:\n",
       "\n",
       "Anderson mixing parameter. Changes G(x) to (1-beta)x + beta G(x). Equivalent to accelerating damped Picard iteration. The history vector is the one for the damped fixed point map, not the original one. Keep this in mind when comparing results.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the fixed point map. Things will go better if you use this rather than hide the data in global variables within the module for your function.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "\\end{itemize}\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = G(solution)       You might want to use functionval as your solution since it's       a Picard iteration applied to the converged Anderson result. If G       is a contraction it will be better than the solution.\n",
       "\n",
       "– history = the vector of residual norms (||x-G(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple (condhist, alphanorm) of the history of the               condition numbers of the optimization problem               and l1 norm of the coefficients.  This is only for diagosing problems and research. Condihist[k] and alphanorm[k] are the condition number and coefficient norm for the optimization problem that computes iteration k+1 from iteration k. \n",
       "\n",
       "I record this for iterations k=1, ... until the final iteration  K. So I do not record the stats for k=0 or the final iteration.  We did record the data for the final iteration in Toth/Kelley  2015 at the cost of an extra optimiztion problem solve.  Since we've already terminated, there's not any point in  collecting that data.\n",
       "\n",
       "Bottom line: if history has length K+1 for iterations  0 ... K, then condhist and alphanorm have length K-1.\n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\\begin{verbatim}\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\\end{verbatim}\n",
       "– solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iterations + 1. \n",
       "\n",
       "\\subsubsection{Examples for aasol}\n",
       "\\paragraph{Duplicate Table 1 from Toth-Kelley 2015.}\n",
       "The final entries in the condition number and coefficient norm statistics are never used in the computation and we don't compute them in Julia. See the docstrings, notebook, and the print book for the story on this.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> function tothk!(G, u)\n",
       "       G[1]=cos(.5*(u[1]+u[2]))\n",
       "       G[2]=G[1]+ 1.e-8 * sin(u[1]*u[1])\n",
       "       return G\n",
       "       end\n",
       "tothk! (generic function with 1 method)\n",
       "\n",
       "julia> u0=ones(2,); m=2; vdim=3*m+3; Vstore = zeros(2, vdim);\n",
       "julia> aout = aasol(tothk!, u0, m, Vstore; rtol = 1.e-10);\n",
       "julia> aout.history\n",
       "8-element Vector{Float64}:\n",
       " 6.50111e-01\n",
       " 4.48661e-01\n",
       " 2.61480e-02\n",
       " 7.25389e-02\n",
       " 1.53107e-04\n",
       " 1.18513e-05\n",
       " 1.82466e-08\n",
       " 1.04725e-13\n",
       "\n",
       "julia> [aout.stats.condhist aout.stats.alphanorm]\n",
       "6×2 Matrix{Float64}:\n",
       " 1.00000e+00  1.00000e+00\n",
       " 2.01556e+10  4.61720e+00\n",
       " 1.37776e+09  2.15749e+00\n",
       " 3.61348e+10  1.18377e+00\n",
       " 2.54948e+11  1.00000e+00\n",
       " 3.67694e+10  1.00171e+00\n",
       "\\end{verbatim}\n",
       "Now we put a mixing or damping paramter in there with beta = .5. This example is nasty enough to make mixing do ok. Keep in mind that the history is for the damped residual, not the original one.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> bout=aasol(tothk!, u0, m, Vstore; rtol = 1.e-10, beta=.5);\n",
       "\n",
       "julia> bout.history\n",
       "7-element Vector{Float64}:\n",
       " 3.25055e-01\n",
       " 3.70140e-02\n",
       " 1.81111e-03\n",
       " 9.55308e-04\n",
       " 1.25936e-05\n",
       " 1.40854e-09\n",
       " 2.18196e-12\n",
       "\\end{verbatim}\n",
       "\\paragraph{H-equation example with m=2. This takes more iterations than}\n",
       "Newton, which should surprise no one.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> n=16; x0=ones(n,); Vstore=zeros(n,20); m=2;\n",
       "julia> hdata=heqinit(x0,.99);\n",
       "julia> hout=aasol(HeqFix!, x0, m, Vstore; pdata=hdata);\n",
       "julia> hout.history\n",
       "12-element Vector{Float64}:\n",
       " 1.47613e+00\n",
       " 7.47800e-01\n",
       " 2.16609e-01\n",
       " 4.32017e-02\n",
       " 2.66867e-02\n",
       " 6.82965e-03\n",
       " 2.70779e-04\n",
       " 6.51027e-05\n",
       " 7.35581e-07\n",
       " 1.85649e-09\n",
       " 4.94803e-10\n",
       " 5.18866e-12\n",
       "\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "aasol(GFix!, x0, m, Vstore; maxit=20,       rtol=1.e-10, atol=1.e-10, beta=1.0, pdata=nothing, keepsolhist = false)\n",
       "\n",
       "C. T. Kelley, 2021\n",
       "\n",
       "Julia code for Anderson acceleration. Nothing fancy.\n",
       "\n",
       "Solvers fixed point problems x = G(x).\n",
       "\n",
       "You must allocate storage for the function and fixed point map history –> in the calling program <– in the array Vstore.\n",
       "\n",
       "For an n dimensional problem with Anderson(m), Vstore must have at least 2m + 4 columns and 3m + 3 is better.  If m=0 (Picard) then V must have at least 4 columns.\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * GFix!: fixed-point map, the ! indicates that GFix! overwrites xout, your   preallocated storage for the function value xout=G(xin).\n",
       "\n",
       "    So xout=GFix!(xout,xin) or xout=GFix!(xout,xin,pdata) returns   xout=G(xin)\n",
       "  * x0: Initial iterate. It is a vector of size N\n",
       "\n",
       "    You should store it as (N,) and design G! to use vectors of size (N,). If you use (N,1) consistently instead, the solvers may work, but I make no guarantees.\n",
       "  * m: depth for Anderson acceleration. m=0 is Picard iteration\n",
       "  * Vstore: Working storage array. For an n dimensional problem Vstore should have at least 3m+3 columns unless you are storage bound. If storage is a problem, then you can allocate a minimum of 2m+4 columns. The smaller allocation exacts a performance penalty, especially for small problems and small values of m. So for Anderson(3), Vstore should be no smaller  than zeros(N,8) with zeros(N,11) a better choice. Vstore needs to allocate for the history of differences of the residuals and fixed point maps. The extra m-1 columns are for storing intermediate results in the downdating phase of the QR factorization for the coefficeint  matrix of the optimization problem. See the notebook or the print book  for the details of this mess. \n",
       "\n",
       "    If m=0, then Vstore needs 4 columns.\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "maxit: default = 20\n",
       "\n",
       "limit on nonlinear iterations\n",
       "\n",
       "rtol and atol: default = 1.e-10\n",
       "\n",
       "relative and absolute error tolerances\n",
       "\n",
       "beta:\n",
       "\n",
       "Anderson mixing parameter. Changes G(x) to (1-beta)x + beta G(x). Equivalent to accelerating damped Picard iteration. The history vector is the one for the damped fixed point map, not the original one. Keep this in mind when comparing results.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the fixed point map. Things will go better if you use this rather than hide the data in global variables within the module for your function.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "Output:\n",
       "\n",
       "  * A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = G(solution)       You might want to use functionval as your solution since it's       a Picard iteration applied to the converged Anderson result. If G       is a contraction it will be better than the solution.\n",
       "\n",
       "– history = the vector of residual norms (||x-G(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple (condhist, alphanorm) of the history of the               condition numbers of the optimization problem               and l1 norm of the coefficients.  This is only for diagosing problems and research. Condihist[k] and alphanorm[k] are the condition number and coefficient norm for the optimization problem that computes iteration k+1 from iteration k. \n",
       "\n",
       "I record this for iterations k=1, ... until the final iteration  K. So I do not record the stats for k=0 or the final iteration.  We did record the data for the final iteration in Toth/Kelley  2015 at the cost of an extra optimiztion problem solve.  Since we've already terminated, there's not any point in  collecting that data.\n",
       "\n",
       "Bottom line: if history has length K+1 for iterations  0 ... K, then condhist and alphanorm have length K-1.\n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if if the iteration succeeded\n",
       "\n",
       "```\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "```\n",
       "\n",
       "– solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iterations + 1. \n",
       "\n",
       "### Examples for aasol\n",
       "\n",
       "#### Duplicate Table 1 from Toth-Kelley 2015.\n",
       "\n",
       "The final entries in the condition number and coefficient norm statistics are never used in the computation and we don't compute them in Julia. See the docstrings, notebook, and the print book for the story on this.\n",
       "\n",
       "```jldoctest\n",
       "julia> function tothk!(G, u)\n",
       "       G[1]=cos(.5*(u[1]+u[2]))\n",
       "       G[2]=G[1]+ 1.e-8 * sin(u[1]*u[1])\n",
       "       return G\n",
       "       end\n",
       "tothk! (generic function with 1 method)\n",
       "\n",
       "julia> u0=ones(2,); m=2; vdim=3*m+3; Vstore = zeros(2, vdim);\n",
       "julia> aout = aasol(tothk!, u0, m, Vstore; rtol = 1.e-10);\n",
       "julia> aout.history\n",
       "8-element Vector{Float64}:\n",
       " 6.50111e-01\n",
       " 4.48661e-01\n",
       " 2.61480e-02\n",
       " 7.25389e-02\n",
       " 1.53107e-04\n",
       " 1.18513e-05\n",
       " 1.82466e-08\n",
       " 1.04725e-13\n",
       "\n",
       "julia> [aout.stats.condhist aout.stats.alphanorm]\n",
       "6×2 Matrix{Float64}:\n",
       " 1.00000e+00  1.00000e+00\n",
       " 2.01556e+10  4.61720e+00\n",
       " 1.37776e+09  2.15749e+00\n",
       " 3.61348e+10  1.18377e+00\n",
       " 2.54948e+11  1.00000e+00\n",
       " 3.67694e+10  1.00171e+00\n",
       "```\n",
       "\n",
       "Now we put a mixing or damping paramter in there with beta = .5. This example is nasty enough to make mixing do ok. Keep in mind that the history is for the damped residual, not the original one.\n",
       "\n",
       "```\n",
       "julia> bout=aasol(tothk!, u0, m, Vstore; rtol = 1.e-10, beta=.5);\n",
       "\n",
       "julia> bout.history\n",
       "7-element Vector{Float64}:\n",
       " 3.25055e-01\n",
       " 3.70140e-02\n",
       " 1.81111e-03\n",
       " 9.55308e-04\n",
       " 1.25936e-05\n",
       " 1.40854e-09\n",
       " 2.18196e-12\n",
       "```\n",
       "\n",
       "#### H-equation example with m=2. This takes more iterations than\n",
       "\n",
       "Newton, which should surprise no one.\n",
       "\n",
       "```jldoctest\n",
       "julia> n=16; x0=ones(n,); Vstore=zeros(n,20); m=2;\n",
       "julia> hdata=heqinit(x0,.99);\n",
       "julia> hout=aasol(HeqFix!, x0, m, Vstore; pdata=hdata);\n",
       "julia> hout.history\n",
       "12-element Vector{Float64}:\n",
       " 1.47613e+00\n",
       " 7.47800e-01\n",
       " 2.16609e-01\n",
       " 4.32017e-02\n",
       " 2.66867e-02\n",
       " 6.82965e-03\n",
       " 2.70779e-04\n",
       " 6.51027e-05\n",
       " 7.35581e-07\n",
       " 1.85649e-09\n",
       " 4.94803e-10\n",
       " 5.18866e-12\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "  aasol(GFix!, x0, m, Vstore; maxit=20, rtol=1.e-10, atol=1.e-10, beta=1.0,\n",
       "  pdata=nothing, keepsolhist = false)\n",
       "\n",
       "  C. T. Kelley, 2021\n",
       "\n",
       "  Julia code for Anderson acceleration. Nothing fancy.\n",
       "\n",
       "  Solvers fixed point problems x = G(x).\n",
       "\n",
       "  You must allocate storage for the function and fixed point map history –> in\n",
       "  the calling program <– in the array Vstore.\n",
       "\n",
       "  For an n dimensional problem with Anderson(m), Vstore must have at least 2m\n",
       "  + 4 columns and 3m + 3 is better. If m=0 (Picard) then V must have at least\n",
       "  4 columns.\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •  GFix!: fixed-point map, the ! indicates that GFix! overwrites\n",
       "       xout, your preallocated storage for the function value\n",
       "       xout=G(xin).\n",
       "       So xout=GFix!(xout,xin) or xout=GFix!(xout,xin,pdata) returns\n",
       "       xout=G(xin)\n",
       "\n",
       "    •  x0: Initial iterate. It is a vector of size N\n",
       "       You should store it as (N,) and design G! to use vectors of size\n",
       "       (N,). If you use (N,1) consistently instead, the solvers may work,\n",
       "       but I make no guarantees.\n",
       "\n",
       "    •  m: depth for Anderson acceleration. m=0 is Picard iteration\n",
       "\n",
       "    •  Vstore: Working storage array. For an n dimensional problem Vstore\n",
       "       should have at least 3m+3 columns unless you are storage bound. If\n",
       "       storage is a problem, then you can allocate a minimum of 2m+4\n",
       "       columns. The smaller allocation exacts a performance penalty,\n",
       "       especially for small problems and small values of m. So for\n",
       "       Anderson(3), Vstore should be no smaller than zeros(N,8) with\n",
       "       zeros(N,11) a better choice. Vstore needs to allocate for the\n",
       "       history of differences of the residuals and fixed point maps. The\n",
       "       extra m-1 columns are for storing intermediate results in the\n",
       "       downdating phase of the QR factorization for the coefficeint\n",
       "       matrix of the optimization problem. See the notebook or the print\n",
       "       book for the details of this mess.\n",
       "       If m=0, then Vstore needs 4 columns.\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  maxit: default = 20\n",
       "\n",
       "  limit on nonlinear iterations\n",
       "\n",
       "  rtol and atol: default = 1.e-10\n",
       "\n",
       "  relative and absolute error tolerances\n",
       "\n",
       "  beta:\n",
       "\n",
       "  Anderson mixing parameter. Changes G(x) to (1-beta)x + beta G(x). Equivalent\n",
       "  to accelerating damped Picard iteration. The history vector is the one for\n",
       "  the damped fixed point map, not the original one. Keep this in mind when\n",
       "  comparing results.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the fixed point map. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  Output:\n",
       "\n",
       "    •  A named tuple (solution, functionval, history, stats, idid,\n",
       "       errcode, solhist)\n",
       "\n",
       "  where\n",
       "\n",
       "  – solution = converged result\n",
       "\n",
       "  – functionval = G(solution) You might want to use functionval as your\n",
       "  solution since it's a Picard iteration applied to the converged Anderson\n",
       "  result. If G is a contraction it will be better than the solution.\n",
       "\n",
       "  – history = the vector of residual norms (||x-G(x)||) for the iteration\n",
       "\n",
       "  – stats = named tuple (condhist, alphanorm) of the history of the condition\n",
       "  numbers of the optimization problem and l1 norm of the coefficients. This is\n",
       "  only for diagosing problems and research. Condihist[k] and alphanorm[k] are\n",
       "  the condition number and coefficient norm for the optimization problem that\n",
       "  computes iteration k+1 from iteration k.\n",
       "\n",
       "  I record this for iterations k=1, ... until the final iteration K. So I do\n",
       "  not record the stats for k=0 or the final iteration. We did record the data\n",
       "  for the final iteration in Toth/Kelley 2015 at the cost of an extra\n",
       "  optimiztion problem solve. Since we've already terminated, there's not any\n",
       "  point in collecting that data.\n",
       "\n",
       "  Bottom line: if history has length K+1 for iterations 0 ... K, then condhist\n",
       "  and alphanorm have length K-1.\n",
       "\n",
       "  – idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  – errcode = 0 if if the iteration succeeded\n",
       "\n",
       "\u001b[36m      = -1 if the initial iterate satisfies the termination criteria\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 10 if no convergence after maxit iterations\u001b[39m\n",
       "\n",
       "  – solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iterations + 1.\n",
       "\n",
       "\u001b[1m  Examples for aasol\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[1m  Duplicate Table 1 from Toth-Kelley 2015.\u001b[22m\n",
       "\u001b[1m  ------------------------------------------\u001b[22m\n",
       "\n",
       "  The final entries in the condition number and coefficient norm statistics\n",
       "  are never used in the computation and we don't compute them in Julia. See\n",
       "  the docstrings, notebook, and the print book for the story on this.\n",
       "\n",
       "\u001b[36m  julia> function tothk!(G, u)\u001b[39m\n",
       "\u001b[36m         G[1]=cos(.5*(u[1]+u[2]))\u001b[39m\n",
       "\u001b[36m         G[2]=G[1]+ 1.e-8 * sin(u[1]*u[1])\u001b[39m\n",
       "\u001b[36m         return G\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  tothk! (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> u0=ones(2,); m=2; vdim=3*m+3; Vstore = zeros(2, vdim);\u001b[39m\n",
       "\u001b[36m  julia> aout = aasol(tothk!, u0, m, Vstore; rtol = 1.e-10);\u001b[39m\n",
       "\u001b[36m  julia> aout.history\u001b[39m\n",
       "\u001b[36m  8-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   6.50111e-01\u001b[39m\n",
       "\u001b[36m   4.48661e-01\u001b[39m\n",
       "\u001b[36m   2.61480e-02\u001b[39m\n",
       "\u001b[36m   7.25389e-02\u001b[39m\n",
       "\u001b[36m   1.53107e-04\u001b[39m\n",
       "\u001b[36m   1.18513e-05\u001b[39m\n",
       "\u001b[36m   1.82466e-08\u001b[39m\n",
       "\u001b[36m   1.04725e-13\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [aout.stats.condhist aout.stats.alphanorm]\u001b[39m\n",
       "\u001b[36m  6×2 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   1.00000e+00  1.00000e+00\u001b[39m\n",
       "\u001b[36m   2.01556e+10  4.61720e+00\u001b[39m\n",
       "\u001b[36m   1.37776e+09  2.15749e+00\u001b[39m\n",
       "\u001b[36m   3.61348e+10  1.18377e+00\u001b[39m\n",
       "\u001b[36m   2.54948e+11  1.00000e+00\u001b[39m\n",
       "\u001b[36m   3.67694e+10  1.00171e+00\u001b[39m\n",
       "\n",
       "  Now we put a mixing or damping paramter in there with beta = .5. This\n",
       "  example is nasty enough to make mixing do ok. Keep in mind that the history\n",
       "  is for the damped residual, not the original one.\n",
       "\n",
       "\u001b[36m  julia> bout=aasol(tothk!, u0, m, Vstore; rtol = 1.e-10, beta=.5);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> bout.history\u001b[39m\n",
       "\u001b[36m  7-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   3.25055e-01\u001b[39m\n",
       "\u001b[36m   3.70140e-02\u001b[39m\n",
       "\u001b[36m   1.81111e-03\u001b[39m\n",
       "\u001b[36m   9.55308e-04\u001b[39m\n",
       "\u001b[36m   1.25936e-05\u001b[39m\n",
       "\u001b[36m   1.40854e-09\u001b[39m\n",
       "\u001b[36m   2.18196e-12\u001b[39m\n",
       "\n",
       "\u001b[1m  H-equation example with m=2. This takes more iterations than\u001b[22m\n",
       "\u001b[1m  --------------------------------------------------------------\u001b[22m\n",
       "\n",
       "  Newton, which should surprise no one.\n",
       "\n",
       "\u001b[36m  julia> n=16; x0=ones(n,); Vstore=zeros(n,20); m=2;\u001b[39m\n",
       "\u001b[36m  julia> hdata=heqinit(x0,.99);\u001b[39m\n",
       "\u001b[36m  julia> hout=aasol(HeqFix!, x0, m, Vstore; pdata=hdata);\u001b[39m\n",
       "\u001b[36m  julia> hout.history\u001b[39m\n",
       "\u001b[36m  12-element Vector{Float64}:\u001b[39m\n",
       "\u001b[36m   1.47613e+00\u001b[39m\n",
       "\u001b[36m   7.47800e-01\u001b[39m\n",
       "\u001b[36m   2.16609e-01\u001b[39m\n",
       "\u001b[36m   4.32017e-02\u001b[39m\n",
       "\u001b[36m   2.66867e-02\u001b[39m\n",
       "\u001b[36m   6.82965e-03\u001b[39m\n",
       "\u001b[36m   2.70779e-04\u001b[39m\n",
       "\u001b[36m   6.51027e-05\u001b[39m\n",
       "\u001b[36m   7.35581e-07\u001b[39m\n",
       "\u001b[36m   1.85649e-09\u001b[39m\n",
       "\u001b[36m   4.94803e-10\u001b[39m\n",
       "\u001b[36m   5.18866e-12\u001b[39m\n",
       "\u001b[36m  \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? aasol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
